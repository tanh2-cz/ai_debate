"""
åŠ¨æ€RAGæ¨¡å— - å®æ—¶æ£€ç´¢æƒå¨æ•°æ®åº“
æ”¯æŒarXivã€CrossRefç­‰å­¦æœ¯æ•°æ®æºçš„çœŸå®æ£€ç´¢
é›†æˆä¸­æ–‡æŸ¥è¯¢æ™ºèƒ½ç¿»è¯‘åŠŸèƒ½
"""

import os
import asyncio
import aiohttp
import requests
import xml.etree.ElementTree as ET
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import hashlib
import json
import time

from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_deepseek import ChatDeepSeek
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# é…ç½®
RAG_CONFIG = {
    "max_results_per_source": 5,
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "similarity_threshold": 0.7,
    "cache_duration_hours": 24,
    "embedding_model": "sentence-transformers/all-MiniLM-L6-v2"
}

@dataclass
class SearchResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç±»"""
    title: str
    authors: List[str]
    abstract: str
    url: str
    published_date: str
    source: str
    relevance_score: float = 0.0
    key_findings: str = ""

class RAGCache:
    """RAGç»“æœç¼“å­˜ç®¡ç†"""
    
    def __init__(self, cache_dir: str = "./rag_cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def _get_cache_key(self, query: str, sources: List[str]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_string = f"{query}_{'-'.join(sorted(sources))}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def get_cached_results(self, query: str, sources: List[str]) -> Optional[List[SearchResult]]:
        """è·å–ç¼“å­˜çš„æ£€ç´¢ç»“æœ"""
        cache_key = self._get_cache_key(query, sources)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        
        if not os.path.exists(cache_file):
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            cache_time = datetime.fromisoformat(cache_data['timestamp'])
            if datetime.now() - cache_time > timedelta(hours=RAG_CONFIG['cache_duration_hours']):
                os.remove(cache_file)
                return None
            
            # é‡æ„SearchResultå¯¹è±¡
            results = []
            for item in cache_data['results']:
                results.append(SearchResult(**item))
            
            return results
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜è¯»å–é”™è¯¯: {e}")
            return None
    
    def cache_results(self, query: str, sources: List[str], results: List[SearchResult]):
        """ç¼“å­˜æ£€ç´¢ç»“æœ"""
        cache_key = self._get_cache_key(query, sources)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        
        try:
            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'query': query,
                'sources': sources,
                'results': [result.__dict__ for result in results]
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âŒ ç¼“å­˜å†™å…¥é”™è¯¯: {e}")

class QueryTranslator:
    """æŸ¥è¯¢ç¿»è¯‘å™¨ - å°†ä¸­æ–‡è¾©è®ºä¸»é¢˜è½¬æ¢ä¸ºé€‚åˆå­¦æœ¯æ£€ç´¢çš„è‹±æ–‡æŸ¥è¯¢"""
    
    def __init__(self, llm: ChatDeepSeek):
        self.llm = llm
        self.translation_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯æ£€ç´¢æŸ¥è¯¢ç¿»è¯‘ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ä¸­æ–‡è¾©è®ºä¸»é¢˜è½¬æ¢ä¸ºé€‚åˆåœ¨arXivã€CrossRefç­‰å›½é™…å­¦æœ¯æ•°æ®åº“ä¸­æœç´¢çš„è‹±æ–‡å…³é”®è¯ã€‚

ç¿»è¯‘è¦æ±‚ï¼š
1. æå–æ ¸å¿ƒå­¦æœ¯æ¦‚å¿µï¼Œè½¬æ¢ä¸ºæ ‡å‡†è‹±æ–‡å­¦æœ¯æœ¯è¯­
2. å»é™¤è¾©è®ºæ€§è¯­è¨€ï¼Œä¿ç•™å®¢è§‚ç ”ç©¶ä¸»é¢˜
3. ä½¿ç”¨ç®€æ´çš„å…³é”®è¯ç»„åˆï¼Œä¸è¦å®Œæ•´å¥å­
4. ä¼˜å…ˆä½¿ç”¨åœ¨å­¦æœ¯ç•Œé€šç”¨çš„è‹±æ–‡æœ¯è¯­
5. è€ƒè™‘åŒä¹‰è¯å’Œç›¸å…³æ¦‚å¿µ

ç¤ºä¾‹ï¼š
ä¸­æ–‡ï¼š"äººå·¥æ™ºèƒ½æ˜¯å¦ä¼šå¨èƒäººç±»å°±ä¸šï¼Ÿ"
è‹±æ–‡ï¼š"artificial intelligence employment impact automation job displacement"

ä¸­æ–‡ï¼š"æ ¸èƒ½å‘ç”µæ˜¯è§£å†³æ°”å€™å˜åŒ–çš„æœ€ä½³æ–¹æ¡ˆå—ï¼Ÿ"
è‹±æ–‡ï¼š"nuclear power climate change mitigation renewable energy policy"

è¯·ä¸ºä»¥ä¸‹ä¸­æ–‡ä¸»é¢˜æä¾›æœ€ä½³çš„è‹±æ–‡å­¦æœ¯æ£€ç´¢æŸ¥è¯¢ï¼š"""),
            ("user", "ä¸­æ–‡ä¸»é¢˜ï¼š{chinese_topic}\n\nè¯·æä¾›3-6ä¸ªæ ¸å¿ƒè‹±æ–‡å…³é”®è¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼š")
        ])
    
    def translate_to_academic_query(self, chinese_topic: str) -> str:
        """å°†ä¸­æ–‡ä¸»é¢˜ç¿»è¯‘ä¸ºè‹±æ–‡å­¦æœ¯æŸ¥è¯¢"""
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯è‹±æ–‡
            if self._is_english(chinese_topic):
                return self._extract_keywords(chinese_topic)
            
            pipe = self.translation_prompt | self.llm | StrOutputParser()
            
            english_query = pipe.invoke({"chinese_topic": chinese_topic})
            
            # æ¸…ç†å“åº”ï¼Œæå–å…³é”®è¯
            keywords = self._clean_query_response(english_query)
            
            print(f"ğŸ”¤ æŸ¥è¯¢ç¿»è¯‘: '{chinese_topic}' -> '{keywords}'")
            return keywords
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢ç¿»è¯‘å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•ç¿»è¯‘
            return self._fallback_translation(chinese_topic)
    
    def _is_english(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸»è¦ä¸ºè‹±æ–‡"""
        english_chars = sum(1 for c in text if c.isascii() and c.isalpha())
        total_chars = sum(1 for c in text if c.isalpha())
        return total_chars > 0 and english_chars / total_chars > 0.7
    
    def _extract_keywords(self, english_text: str) -> str:
        """ä»è‹±æ–‡æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        # ç§»é™¤å¸¸è§åœç”¨è¯å’Œç–‘é—®è¯
        stop_words = {'is', 'are', 'will', 'can', 'should', 'does', 'do', 'the', 'a', 'an', 
                     'how', 'what', 'why', 'where', 'when', 'which', 'whether', 'if'}
        
        words = english_text.lower().replace('?', '').replace('!', '').split()
        keywords = [word.strip('.,!?()[]{}') for word in words 
                   if word.lower() not in stop_words and len(word) > 2]
        
        return ' '.join(keywords[:6])  # é™åˆ¶å…³é”®è¯æ•°é‡
    
    def _clean_query_response(self, response: str) -> str:
        """æ¸…ç†LLMå“åº”ï¼Œæå–å…³é”®è¯"""
        # ç§»é™¤å¯èƒ½çš„è§£é‡Šæ–‡å­—ï¼Œåªä¿ç•™å…³é”®è¯
        lines = response.strip().split('\n')
        for line in lines:
            if ':' in line:
                # å¦‚æœæœ‰å†’å·ï¼Œå–å†’å·åçš„éƒ¨åˆ†
                keywords = line.split(':', 1)[1].strip()
            else:
                keywords = line.strip()
            
            # éªŒè¯æ˜¯å¦æ˜¯å…³é”®è¯æ ¼å¼
            if len(keywords.split()) <= 8 and not keywords.startswith('ä¸­æ–‡') and not keywords.startswith('è‹±æ–‡'):
                return keywords
        
        # å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚çš„è¡Œï¼Œè¿”å›æ•´ä¸ªå“åº”çš„å‰6ä¸ªè¯
        words = response.replace('\n', ' ').split()
        return ' '.join(words[:6])
    
    def _fallback_translation(self, chinese_topic: str) -> str:
        """å›é€€ç¿»è¯‘æ–¹æ¡ˆ"""
        # ç®€å•çš„è¯æ±‡æ˜ å°„
        translation_map = {
            'äººå·¥æ™ºèƒ½': 'artificial intelligence',
            'æœºå™¨å­¦ä¹ ': 'machine learning',
            'æ·±åº¦å­¦ä¹ ': 'deep learning',
            'å°±ä¸š': 'employment',
            'å·¥ä½œ': 'job work',
            'æ ¸èƒ½': 'nuclear power',
            'æ°”å€™å˜åŒ–': 'climate change',
            'ç¯å¢ƒä¿æŠ¤': 'environmental protection',
            'ç»æµ': 'economics economy',
            'æ”¿ç­–': 'policy',
            'æŠ€æœ¯': 'technology',
            'ç¤¾ä¼š': 'society social',
            'ä¼¦ç†': 'ethics',
            'é“å¾·': 'moral',
            'å¯æŒç»­å‘å±•': 'sustainable development',
            'èƒ½æº': 'energy',
            'æ•™è‚²': 'education',
            'åŒ»ç–—': 'healthcare medical',
            'è‡ªåŠ¨åŒ–': 'automation',
            'æœºå™¨äºº': 'robotics',
            'ç”Ÿç‰©æŠ€æœ¯': 'biotechnology',
            'åŸºå› ç¼–è¾‘': 'gene editing CRISPR',
            'åŒºå—é“¾': 'blockchain',
            'é‡å­è®¡ç®—': 'quantum computing',
            'è‡ªåŠ¨é©¾é©¶': 'autonomous driving',
            'è™šæ‹Ÿç°å®': 'virtual reality',
            'å¢å¼ºç°å®': 'augmented reality'
        }
        
        keywords = []
        for chinese, english in translation_map.items():
            if chinese in chinese_topic:
                keywords.extend(english.split())
        
        if not keywords:
            # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œä½¿ç”¨é€šç”¨ç ”ç©¶è¯æ±‡
            keywords = ['research', 'analysis', 'study', 'technology', 'society']
        
        return ' '.join(keywords[:6])

class ArxivSearcher:
    """arXivå­¦æœ¯è®ºæ–‡æ£€ç´¢å™¨"""
    
    def __init__(self):
        self.base_url = "http://export.arxiv.org/api/query"
        self.session = requests.Session()
    
    def search(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """æ£€ç´¢arXivè®ºæ–‡"""
        try:
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            params = {
                'search_query': f'all:{query}',
                'start': 0,
                'max_results': max_results,
                'sortBy': 'relevance',
                'sortOrder': 'descending'
            }
            
            print(f"ğŸ” æ­£åœ¨arXivæ£€ç´¢: {query}")
            response = self.session.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            return self._parse_arxiv_response(response.text)
            
        except Exception as e:
            print(f"âŒ arXivæ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_arxiv_response(self, xml_content: str) -> List[SearchResult]:
        """è§£æarXiv APIå“åº”"""
        results = []
        
        try:
            root = ET.fromstring(xml_content)
            namespace = {'atom': 'http://www.w3.org/2005/Atom'}
            
            for entry in root.findall('atom:entry', namespace):
                title = entry.find('atom:title', namespace)
                title = title.text.strip() if title is not None else "æ— æ ‡é¢˜"
                
                # ä½œè€…ä¿¡æ¯
                authors = []
                for author in entry.findall('atom:author', namespace):
                    name = author.find('atom:name', namespace)
                    if name is not None:
                        authors.append(name.text.strip())
                
                # æ‘˜è¦
                summary = entry.find('atom:summary', namespace)
                abstract = summary.text.strip() if summary is not None else "æ— æ‘˜è¦"
                
                # URL
                link = entry.find('atom:id', namespace)
                url = link.text.strip() if link is not None else ""
                
                # å‘å¸ƒæ—¶é—´
                published = entry.find('atom:published', namespace)
                published_date = published.text[:10] if published is not None else ""
                
                result = SearchResult(
                    title=title,
                    authors=authors,
                    abstract=abstract,
                    url=url,
                    published_date=published_date,
                    source="arXiv"
                )
                
                results.append(result)
                
        except ET.ParseError as e:
            print(f"âŒ arXivå“åº”è§£æé”™è¯¯: {e}")
        
        return results

class CrossRefSearcher:
    """CrossRefæœŸåˆŠæ–‡ç« æ£€ç´¢å™¨"""
    
    def __init__(self):
        self.base_url = "https://api.crossref.org/works"
        self.session = requests.Session()
        # è®¾ç½®ç”¨æˆ·ä»£ç†é¿å…è¢«é™åˆ¶
        self.session.headers.update({
            'User-Agent': 'Multi-Agent-Debate-Platform/1.0 (mailto:admin@example.com)'
        })
    
    def search(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """æ£€ç´¢æœŸåˆŠæ–‡ç« """
        try:
            params = {
                'query': query,
                'rows': max_results,
                'sort': 'relevance',
                'filter': 'type:journal-article',
                'select': 'title,author,abstract,URL,published-print,container-title'
            }
            
            print(f"ğŸ” æ­£åœ¨CrossRefæ£€ç´¢: {query}")
            response = self.session.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            return self._parse_crossref_response(response.json())
            
        except Exception as e:
            print(f"âŒ CrossRefæ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_crossref_response(self, data: dict) -> List[SearchResult]:
        """è§£æCrossRef APIå“åº”"""
        results = []
        
        try:
            items = data.get('message', {}).get('items', [])
            
            for item in items:
                # æ ‡é¢˜
                title_list = item.get('title', [])
                title = title_list[0] if title_list else "æ— æ ‡é¢˜"
                
                # ä½œè€…
                authors = []
                author_list = item.get('author', [])
                for author in author_list:
                    given = author.get('given', '')
                    family = author.get('family', '')
                    if given and family:
                        authors.append(f"{given} {family}")
                    elif family:
                        authors.append(family)
                
                # æ‘˜è¦ï¼ˆCrossRefé€šå¸¸ä¸æä¾›å®Œæ•´æ‘˜è¦ï¼‰
                abstract = item.get('abstract', 'æ‘˜è¦ä¿¡æ¯éœ€è¦è®¿é—®åŸæ–‡')
                
                # URL
                url = item.get('URL', '')
                
                # å‘å¸ƒæ—¶é—´
                published_date = ""
                date_parts = item.get('published-print', {}).get('date-parts', [])
                if date_parts and len(date_parts[0]) >= 3:
                    year, month, day = date_parts[0][:3]
                    published_date = f"{year}-{month:02d}-{day:02d}"
                elif date_parts and len(date_parts[0]) >= 1:
                    published_date = str(date_parts[0][0])
                
                result = SearchResult(
                    title=title,
                    authors=authors,
                    abstract=abstract,
                    url=url,
                    published_date=published_date,
                    source="CrossRef"
                )
                
                results.append(result)
                
        except Exception as e:
            print(f"âŒ CrossRefå“åº”è§£æé”™è¯¯: {e}")
        
        return results

class RAGEnhancer:
    """RAGå¢å¼ºå™¨ - å¤„ç†æ£€ç´¢ç»“æœå¹¶ç”Ÿæˆæ´å¯Ÿ"""
    
    def __init__(self, llm: ChatDeepSeek):
        self.llm = llm
        self.analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯ç ”ç©¶åˆ†æä¸“å®¶ã€‚åŸºäºç»™å®šçš„å­¦æœ¯è®ºæ–‡ä¿¡æ¯ï¼Œæå–å’Œæ€»ç»“å…³é”®å‘ç°ï¼Œä¸ºè¾©è®ºæä¾›æ”¯æ’‘ã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. åˆ†æè®ºæ–‡çš„æ ¸å¿ƒè§‚ç‚¹å’Œå‘ç°
2. æå–ä¸è¾©è®ºä¸»é¢˜ç›¸å…³çš„å…³é”®è¯æ®
3. ç®€æ´åœ°æ€»ç»“ä¸»è¦è®ºç‚¹ï¼ˆ2-3å¥è¯ï¼‰
4. è¯„ä¼°ç ”ç©¶çš„å¯ä¿¡åº¦å’Œç›¸å…³æ€§

è®ºæ–‡ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{title}
ä½œè€…ï¼š{authors}
æ‘˜è¦ï¼š{abstract}
å‘å¸ƒæ—¶é—´ï¼š{published_date}
æ¥æºï¼š{source}

è¾©è®ºä¸»é¢˜ï¼š{debate_topic}

è¯·æä¾›ï¼š
1. å…³é”®å‘ç°ï¼ˆæ ¸å¿ƒè§‚ç‚¹å’Œè¯æ®ï¼‰
2. ä¸è¾©è®ºä¸»é¢˜çš„ç›¸å…³æ€§è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰
3. å»ºè®®åœ¨è¾©è®ºä¸­å¦‚ä½•å¼•ç”¨è¿™é¡¹ç ”ç©¶"""),
            ("user", "è¯·åˆ†æè¿™ç¯‡è®ºæ–‡å¹¶æä¾›å…³é”®æ´å¯Ÿ")
        ])
    
    def enhance_results(self, results: List[SearchResult], debate_topic: str) -> List[SearchResult]:
        """å¢å¼ºæ£€ç´¢ç»“æœï¼Œæå–å…³é”®æ´å¯Ÿ"""
        enhanced_results = []
        
        for result in results:
            try:
                # ä½¿ç”¨LLMåˆ†æè®ºæ–‡
                analysis = self._analyze_paper(result, debate_topic)
                result.key_findings = analysis.get('key_findings', '')
                result.relevance_score = analysis.get('relevance_score', 5.0)
                enhanced_results.append(result)
                
                # é¿å…APIé™åˆ¶
                time.sleep(1)
                
            except Exception as e:
                print(f"âŒ è®ºæ–‡åˆ†æå¤±è´¥ {result.title}: {e}")
                # å³ä½¿åˆ†æå¤±è´¥ä¹Ÿä¿ç•™åŸå§‹ç»“æœ
                enhanced_results.append(result)
        
        # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
        enhanced_results.sort(key=lambda x: x.relevance_score, reverse=True)
        return enhanced_results
    
    def _analyze_paper(self, result: SearchResult, debate_topic: str) -> dict:
        """åˆ†æå•ç¯‡è®ºæ–‡"""
        try:
            pipe = self.analysis_prompt | self.llm | StrOutputParser()
            
            response = pipe.invoke({
                'title': result.title,
                'authors': ', '.join(result.authors[:3]),  # é™åˆ¶ä½œè€…æ•°é‡
                'abstract': result.abstract[:1000],  # é™åˆ¶æ‘˜è¦é•¿åº¦
                'published_date': result.published_date,
                'source': result.source,
                'debate_topic': debate_topic
            })
            
            # ç®€å•è§£æå“åº”
            lines = response.strip().split('\n')
            key_findings = ""
            relevance_score = 5.0
            
            for line in lines:
                if 'å…³é”®å‘ç°' in line or 'æ ¸å¿ƒè§‚ç‚¹' in line:
                    key_findings = line.split('ï¼š', 1)[-1].strip()
                elif 'ç›¸å…³æ€§' in line and 'åˆ†' in line:
                    try:
                        # å°è¯•æå–æ•°å­—
                        import re
                        score_match = re.search(r'(\d+)', line)
                        if score_match:
                            relevance_score = float(score_match.group(1))
                    except:
                        pass
            
            return {
                'key_findings': key_findings or response[:200],
                'relevance_score': min(max(relevance_score, 1.0), 10.0)
            }
            
        except Exception as e:
            print(f"âŒ LLMåˆ†æé”™è¯¯: {e}")
            return {
                'key_findings': result.abstract[:150] + "...",
                'relevance_score': 5.0
            }

class DynamicRAGModule:
    """åŠ¨æ€RAGä¸»æ¨¡å—"""
    
    def __init__(self, llm: ChatDeepSeek):
        self.llm = llm
        self.cache = RAGCache()
        self.arxiv_searcher = ArxivSearcher()
        self.crossref_searcher = CrossRefSearcher()
        self.enhancer = RAGEnhancer(llm)
        self.query_translator = QueryTranslator(llm)  # æ–°å¢æŸ¥è¯¢ç¿»è¯‘å™¨
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨ï¼ˆå¯é€‰ï¼Œç”¨äºæ›´å¤æ‚çš„ç›¸ä¼¼æ€§æ£€ç´¢ï¼‰
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name=RAG_CONFIG["embedding_model"]
            )
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.embeddings = None
    
    def search_academic_sources(self, 
                              topic: str, 
                              sources: List[str] = ["arxiv", "crossref"],
                              max_results_per_source: int = None) -> List[SearchResult]:
        """æœç´¢å­¦æœ¯æ•°æ®æºï¼ˆæ”¯æŒä¸­æ–‡æŸ¥è¯¢è‡ªåŠ¨ç¿»è¯‘ï¼‰"""
        
        if max_results_per_source is None:
            max_results_per_source = RAG_CONFIG["max_results_per_source"]
        
        # å°†ä¸­æ–‡ä¸»é¢˜ç¿»è¯‘ä¸ºè‹±æ–‡å­¦æœ¯æŸ¥è¯¢
        english_query = self.query_translator.translate_to_academic_query(topic)
        
        # æ£€æŸ¥ç¼“å­˜ï¼ˆä½¿ç”¨è‹±æ–‡æŸ¥è¯¢ä½œä¸ºç¼“å­˜é”®ï¼‰
        cached_results = self.cache.get_cached_results(english_query, sources)
        if cached_results:
            print(f"âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ: {len(cached_results)} ç¯‡è®ºæ–‡")
            return cached_results
        
        all_results = []
        
        # arXivæ£€ç´¢
        if "arxiv" in sources:
            print(f"ğŸ” arXivæ£€ç´¢ä¸­æ–‡ä¸»é¢˜: {topic}")
            print(f"ğŸ” arXivä½¿ç”¨è‹±æ–‡æŸ¥è¯¢: {english_query}")
            arxiv_results = self.arxiv_searcher.search(english_query, max_results_per_source)
            all_results.extend(arxiv_results)
            print(f"ğŸ“š arXivæ‰¾åˆ° {len(arxiv_results)} ç¯‡è®ºæ–‡")
        
        # CrossRefæ£€ç´¢
        if "crossref" in sources:
            print(f"ğŸ” CrossRefæ£€ç´¢ä¸­æ–‡ä¸»é¢˜: {topic}")
            print(f"ğŸ” CrossRefä½¿ç”¨è‹±æ–‡æŸ¥è¯¢: {english_query}")
            crossref_results = self.crossref_searcher.search(english_query, max_results_per_source)
            all_results.extend(crossref_results)
            print(f"ğŸ“š CrossRefæ‰¾åˆ° {len(crossref_results)} ç¯‡è®ºæ–‡")
        
        # ä½¿ç”¨LLMå¢å¼ºç»“æœï¼ˆä½¿ç”¨åŸå§‹ä¸­æ–‡ä¸»é¢˜è¿›è¡Œç›¸å…³æ€§è¯„ä¼°ï¼‰
        if all_results and self.llm:
            print("ğŸ¤– ä½¿ç”¨AIåˆ†æè®ºæ–‡ç›¸å…³æ€§...")
            all_results = self.enhancer.enhance_results(all_results, topic)  # ä½¿ç”¨ä¸­æ–‡ä¸»é¢˜è¯„ä¼°ç›¸å…³æ€§
        
        # ç¼“å­˜ç»“æœï¼ˆä½¿ç”¨è‹±æ–‡æŸ¥è¯¢ä½œä¸ºç¼“å­˜é”®ï¼‰
        if all_results:
            self.cache.cache_results(english_query, sources, all_results)
        
        return all_results
    
    def get_rag_context_for_agent(self, 
                                 agent_role: str, 
                                 debate_topic: str, 
                                 max_sources: int = 3) -> str:
        """ä¸ºç‰¹å®šè§’è‰²è·å–RAGä¸Šä¸‹æ–‡ï¼ˆä¼˜åŒ–ç‰ˆï¼Œæ”¯æŒä¸­æ–‡ä¸»é¢˜ï¼‰"""
        
        # åŸºäºè§’è‰²è°ƒæ•´æœç´¢æŸ¥è¯¢ï¼ˆç°åœ¨æ”¯æŒä¸­æ–‡ç¿»è¯‘ï¼‰
        role_focused_query = self._create_role_focused_query(agent_role, debate_topic)
        
        # æ£€ç´¢ç›¸å…³æ–‡çŒ®
        results = self.search_academic_sources(role_focused_query, max_results_per_source=2)
        
        if not results:
            print(f"âš ï¸ æœªæ‰¾åˆ°{agent_role}ç›¸å…³çš„å­¦æœ¯èµ„æ–™")
            return "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚"
        
        # é€‰æ‹©æœ€ç›¸å…³çš„å‡ ç¯‡
        top_results = results[:max_sources]
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, result in enumerate(top_results, 1):
            context_part = f"""
å‚è€ƒèµ„æ–™ {i}:
æ ‡é¢˜: {result.title}
ä½œè€…: {', '.join(result.authors[:2])}
æ¥æº: {result.source} ({result.published_date})
å…³é”®å‘ç°: {result.key_findings or result.abstract[:200]}
ç›¸å…³æ€§: {result.relevance_score}/10
"""
            context_parts.append(context_part.strip())
        
        context = "\n\n".join(context_parts)
        print(f"ğŸ“š ä¸º{agent_role}å‡†å¤‡äº†{len(top_results)}ç¯‡å‚è€ƒæ–‡çŒ®")
        return context
    
    def _create_role_focused_query(self, agent_role: str, debate_topic: str) -> str:
        """åŸºäºè§’è‰²åˆ›å»ºé’ˆå¯¹æ€§æŸ¥è¯¢ï¼ˆä¼˜åŒ–ç‰ˆï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆï¼‰"""
        
        # åŸºç¡€è‹±æ–‡å…³é”®è¯æ˜ å°„ï¼ˆæ ¹æ®ä½ åŸæœ‰çš„è§’è‰²å®šä¹‰ï¼‰
        role_keywords = {
            "environmentalist": "environment climate sustainability ecology conservation renewable",
            "economist": "economic cost benefit market analysis finance policy",
            "policy_maker": "policy governance regulation implementation law public administration",
            "tech_expert": "technology innovation technical feasibility artificial intelligence",
            "sociologist": "social impact society community effects inequality demographics",
            "ethicist": "ethics moral responsibility values philosophy bioethics"
        }
        
        # è·å–è§’è‰²ä¸“ä¸šå…³é”®è¯
        role_english_keywords = role_keywords.get(agent_role, "research analysis")
        
        # å°†ä¸­æ–‡ä¸»é¢˜ç¿»è¯‘ä¸ºè‹±æ–‡
        english_topic = self.query_translator.translate_to_academic_query(debate_topic)
        
        # ç»„åˆæŸ¥è¯¢ï¼šä¸»é¢˜å…³é”®è¯ + è§’è‰²å…³é”®è¯
        combined_query = f"{english_topic} {role_english_keywords}"
        
        # å»é‡å¹¶é™åˆ¶å…³é”®è¯æ•°é‡
        unique_keywords = []
        seen = set()
        for keyword in combined_query.split():
            if keyword.lower() not in seen and len(keyword) > 2:
                unique_keywords.append(keyword)
                seen.add(keyword.lower())
        
        final_query = ' '.join(unique_keywords[:8])  # é™åˆ¶åœ¨8ä¸ªå…³é”®è¯ä»¥å†…
        
        print(f"ğŸ¯ {agent_role}ä¸“ç”¨æŸ¥è¯¢: {final_query}")
        return final_query

# å…¨å±€RAGå®ä¾‹ï¼ˆå°†åœ¨graph.pyä¸­åˆå§‹åŒ–ï¼‰
rag_module = None

def initialize_rag_module(llm: ChatDeepSeek) -> DynamicRAGModule:
    """åˆå§‹åŒ–RAGæ¨¡å—"""
    global rag_module
    rag_module = DynamicRAGModule(llm)
    return rag_module

def get_rag_module() -> Optional[DynamicRAGModule]:
    """è·å–RAGæ¨¡å—å®ä¾‹"""
    return rag_module

# æµ‹è¯•å‡½æ•°
def test_rag_module():
    """æµ‹è¯•RAGæ¨¡å—åŠŸèƒ½ï¼ˆåŒ…æ‹¬ä¸­æ–‡æŸ¥è¯¢ç¿»è¯‘ï¼‰"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•RAGæ¨¡å—...")
    
    # åˆ›å»ºæµ‹è¯•LLMï¼ˆéœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥ï¼‰
    try:
        from langchain_deepseek import ChatDeepSeek
        test_llm = ChatDeepSeek(model="deepseek-chat", temperature=0.3)
        
        # åˆå§‹åŒ–RAGæ¨¡å—
        rag = initialize_rag_module(test_llm)
        
        # æµ‹è¯•ä¸­æ–‡æŸ¥è¯¢ç¿»è¯‘
        print("\nğŸ”¤ æµ‹è¯•æŸ¥è¯¢ç¿»è¯‘åŠŸèƒ½:")
        chinese_topics = [
            "äººå·¥æ™ºèƒ½å¯¹å°±ä¸šçš„å½±å“",
            "æ ¸èƒ½å‘ç”µä¸æ°”å€™å˜åŒ–",
            "åŸºå› ç¼–è¾‘æŠ€æœ¯çš„ä¼¦ç†é—®é¢˜"
        ]
        
        for topic in chinese_topics:
            english_query = rag.query_translator.translate_to_academic_query(topic)
            print(f"  ä¸­æ–‡: {topic}")
            print(f"  è‹±æ–‡: {english_query}\n")
        
        # æµ‹è¯•æ£€ç´¢
        test_topic = "äººå·¥æ™ºèƒ½å¯¹å°±ä¸šçš„å½±å“"
        print(f"ğŸ” æµ‹è¯•æ£€ç´¢ä¸­æ–‡ä¸»é¢˜: {test_topic}")
        results = rag.search_academic_sources(test_topic, sources=["arxiv"])
        
        print(f"âœ… æ£€ç´¢åˆ° {len(results)} ç¯‡ç›¸å…³è®ºæ–‡")
        for i, result in enumerate(results[:2], 1):
            print(f"\nè®ºæ–‡ {i}:")
            print(f"  æ ‡é¢˜: {result.title}")
            print(f"  ç›¸å…³æ€§: {result.relevance_score}/10")
            print(f"  å…³é”®å‘ç°: {result.key_findings[:100]}...")
        
        # æµ‹è¯•è§’è‰²ä¸“ç”¨æŸ¥è¯¢
        print(f"\nğŸ­ æµ‹è¯•è§’è‰²ä¸“ç”¨æŸ¥è¯¢:")
        for role in ["tech_expert", "economist", "sociologist"]:
            context = rag.get_rag_context_for_agent(role, test_topic, max_sources=1)
            print(f"  {role}: {len(context)} å­—ç¬¦çš„ä¸Šä¸‹æ–‡")
            
    except Exception as e:
        print(f"âŒ RAGæ¨¡å—æµ‹è¯•å¤±è´¥: {e}")

def test_query_translation():
    """å•ç‹¬æµ‹è¯•æŸ¥è¯¢ç¿»è¯‘åŠŸèƒ½"""
    print("ğŸ”¤ æµ‹è¯•æŸ¥è¯¢ç¿»è¯‘åŠŸèƒ½...")
    
    try:
        from langchain_deepseek import ChatDeepSeek
        test_llm = ChatDeepSeek(model="deepseek-chat", temperature=0.3)
        translator = QueryTranslator(test_llm)
        
        test_cases = [
            "äººå·¥æ™ºèƒ½æ˜¯å¦ä¼šå¨èƒäººç±»å°±ä¸šï¼Ÿ",
            "æ ¸èƒ½å‘ç”µæ˜¯è§£å†³æ°”å€™å˜åŒ–çš„æœ€ä½³æ–¹æ¡ˆå—ï¼Ÿ",
            "åŸºå› ç¼–è¾‘æŠ€æœ¯çš„ä¼¦ç†è¾¹ç•Œåœ¨å“ªé‡Œï¼Ÿ",
            "è¿œç¨‹å·¥ä½œå¯¹ç¤¾ä¼šç»æµçš„é•¿æœŸå½±å“",
            "artificial intelligence employment impact",  # å·²ç»æ˜¯è‹±æ–‡
            "AIåœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨å‰æ™¯"
        ]
        
        print("æŸ¥è¯¢ç¿»è¯‘æµ‹è¯•ç»“æœ:")
        print("-" * 50)
        for topic in test_cases:
            english_query = translator.translate_to_academic_query(topic)
            print(f"åŸæ–‡: {topic}")
            print(f"è‹±æ–‡: {english_query}")
            print()
            
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢ç¿»è¯‘æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("DEEPSEEK_API_KEY"):
        print("âŒ è­¦å‘Š: DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        print("ğŸ’¡ å¯ä»¥å…ˆæµ‹è¯•æŸ¥è¯¢ç¿»è¯‘çš„åŸºç¡€é€»è¾‘...")
        # test_query_translation()
    else:
        print("âœ… ç¯å¢ƒå˜é‡é…ç½®æ­£ç¡®")
        test_rag_module()
        print("\n" + "="*50)
        test_query_translation()