"""
åŠ¨æ€RAGæ¨¡å— - å®æ—¶æ£€ç´¢æƒå¨æ•°æ®åº“
æ”¯æŒarXivã€CrossRefç­‰å­¦æœ¯æ•°æ®æºçš„çœŸå®æ£€ç´¢
ä¿®å¤ï¼šæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰çš„æ¯ä¸“å®¶æœ€å¤§å‚è€ƒæ–‡çŒ®æ•°è®¾ç½®
ä¼˜åŒ–ï¼šæ”¯æŒåŸºäºä¸“å®¶è§’è‰²çš„ç¼“å­˜æœºåˆ¶
"""

import os
import asyncio
import aiohttp
import requests
import xml.etree.ElementTree as ET
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import hashlib
import json
import time

from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_deepseek import ChatDeepSeek
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# é…ç½®
RAG_CONFIG = {
    "max_results_per_source": 5,
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "similarity_threshold": 0.7,
    "cache_duration_hours": 24,
    "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
    # æ–°å¢ï¼šä¸“å®¶ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
    "agent_cache_duration_hours": 6
}

@dataclass
class SearchResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç±»"""
    title: str
    authors: List[str]
    abstract: str
    url: str
    published_date: str
    source: str
    relevance_score: float = 0.0
    key_findings: str = ""

class RAGCache:
    """RAGç»“æœç¼“å­˜ç®¡ç†ï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒä¸“å®¶è§’è‰²ç¼“å­˜ï¼‰"""
    
    def __init__(self, cache_dir: str = "./rag_cache"):
        self.cache_dir = cache_dir
        self.agent_cache_dir = os.path.join(cache_dir, "agent_cache")
        os.makedirs(cache_dir, exist_ok=True)
        os.makedirs(self.agent_cache_dir, exist_ok=True)
    
    def _get_cache_key(self, query: str, sources: List[str]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_string = f"{query}_{'-'.join(sorted(sources))}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _get_agent_cache_key(self, agent_role: str, debate_topic: str) -> str:
        """ç”Ÿæˆä¸“å®¶è§’è‰²ç‰¹å®šçš„ç¼“å­˜é”®"""
        key_string = f"agent_{agent_role}_{debate_topic}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def get_cached_results(self, query: str, sources: List[str]) -> Optional[List[SearchResult]]:
        """è·å–ç¼“å­˜çš„æ£€ç´¢ç»“æœ"""
        cache_key = self._get_cache_key(query, sources)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        
        if not os.path.exists(cache_file):
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            cache_time = datetime.fromisoformat(cache_data['timestamp'])
            if datetime.now() - cache_time > timedelta(hours=RAG_CONFIG['cache_duration_hours']):
                os.remove(cache_file)
                return None
            
            # é‡æ„SearchResultå¯¹è±¡
            results = []
            for item in cache_data['results']:
                results.append(SearchResult(**item))
            
            return results
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜è¯»å–é”™è¯¯: {e}")
            return None
    
    def cache_results(self, query: str, sources: List[str], results: List[SearchResult]):
        """ç¼“å­˜æ£€ç´¢ç»“æœ"""
        cache_key = self._get_cache_key(query, sources)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        
        try:
            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'query': query,
                'sources': sources,
                'results': [result.__dict__ for result in results]
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âŒ ç¼“å­˜å†™å…¥é”™è¯¯: {e}")
    
    def get_agent_cached_context(self, agent_role: str, debate_topic: str) -> Optional[str]:
        """è·å–ä¸“å®¶è§’è‰²ç‰¹å®šçš„ç¼“å­˜ä¸Šä¸‹æ–‡"""
        cache_key = self._get_agent_cache_key(agent_role, debate_topic)
        cache_file = os.path.join(self.agent_cache_dir, f"{cache_key}.json")
        
        if not os.path.exists(cache_file):
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            cache_time = datetime.fromisoformat(cache_data['timestamp'])
            if datetime.now() - cache_time > timedelta(hours=RAG_CONFIG['agent_cache_duration_hours']):
                os.remove(cache_file)
                return None
            
            return cache_data['context']
            
        except Exception as e:
            print(f"âŒ ä¸“å®¶ç¼“å­˜è¯»å–é”™è¯¯: {e}")
            return None
    
    def cache_agent_context(self, agent_role: str, debate_topic: str, context: str):
        """ç¼“å­˜ä¸“å®¶è§’è‰²ç‰¹å®šçš„ä¸Šä¸‹æ–‡"""
        cache_key = self._get_agent_cache_key(agent_role, debate_topic)
        cache_file = os.path.join(self.agent_cache_dir, f"{cache_key}.json")
        
        try:
            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'agent_role': agent_role,
                'debate_topic': debate_topic,
                'context': context
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
            print(f"âœ… å·²ç¼“å­˜ä¸“å®¶ {agent_role} çš„å­¦æœ¯èµ„æ–™")
                
        except Exception as e:
            print(f"âŒ ä¸“å®¶ç¼“å­˜å†™å…¥é”™è¯¯: {e}")
    
    def clear_agent_cache(self, agent_role: str = None):
        """æ¸…ç†ä¸“å®¶ç¼“å­˜ï¼ˆå¯é€‰æ‹©ç‰¹å®šè§’è‰²ï¼‰"""
        try:
            if agent_role:
                # æ¸…ç†ç‰¹å®šè§’è‰²çš„ç¼“å­˜
                for filename in os.listdir(self.agent_cache_dir):
                    if filename.startswith(f"agent_{agent_role}_"):
                        os.remove(os.path.join(self.agent_cache_dir, filename))
                print(f"âœ… å·²æ¸…ç†ä¸“å®¶ {agent_role} çš„ç¼“å­˜")
            else:
                # æ¸…ç†æ‰€æœ‰ä¸“å®¶ç¼“å­˜
                for filename in os.listdir(self.agent_cache_dir):
                    os.remove(os.path.join(self.agent_cache_dir, filename))
                print("âœ… å·²æ¸…ç†æ‰€æœ‰ä¸“å®¶ç¼“å­˜")
        except Exception as e:
            print(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")

class ArxivSearcher:
    """arXivå­¦æœ¯è®ºæ–‡æ£€ç´¢å™¨"""
    
    def __init__(self):
        self.base_url = "http://export.arxiv.org/api/query"
        self.session = requests.Session()
    
    def search(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """æ£€ç´¢arXivè®ºæ–‡"""
        try:
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            params = {
                'search_query': f'all:{query}',
                'start': 0,
                'max_results': max_results,
                'sortBy': 'relevance',
                'sortOrder': 'descending'
            }
            
            print(f"ğŸ” æ­£åœ¨arXivæ£€ç´¢: {query} (æœ€å¤š{max_results}ç¯‡)")
            response = self.session.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            return self._parse_arxiv_response(response.text)
            
        except Exception as e:
            print(f"âŒ arXivæ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_arxiv_response(self, xml_content: str) -> List[SearchResult]:
        """è§£æarXiv APIå“åº”"""
        results = []
        
        try:
            root = ET.fromstring(xml_content)
            namespace = {'atom': 'http://www.w3.org/2005/Atom'}
            
            for entry in root.findall('atom:entry', namespace):
                title = entry.find('atom:title', namespace)
                title = title.text.strip() if title is not None else "æ— æ ‡é¢˜"
                
                # ä½œè€…ä¿¡æ¯
                authors = []
                for author in entry.findall('atom:author', namespace):
                    name = author.find('atom:name', namespace)
                    if name is not None:
                        authors.append(name.text.strip())
                
                # æ‘˜è¦
                summary = entry.find('atom:summary', namespace)
                abstract = summary.text.strip() if summary is not None else "æ— æ‘˜è¦"
                
                # URL
                link = entry.find('atom:id', namespace)
                url = link.text.strip() if link is not None else ""
                
                # å‘å¸ƒæ—¶é—´
                published = entry.find('atom:published', namespace)
                published_date = published.text[:10] if published is not None else ""
                
                result = SearchResult(
                    title=title,
                    authors=authors,
                    abstract=abstract,
                    url=url,
                    published_date=published_date,
                    source="arXiv"
                )
                
                results.append(result)
                
        except ET.ParseError as e:
            print(f"âŒ arXivå“åº”è§£æé”™è¯¯: {e}")
        
        return results

class CrossRefSearcher:
    """CrossRefæœŸåˆŠæ–‡ç« æ£€ç´¢å™¨"""
    
    def __init__(self):
        self.base_url = "https://api.crossref.org/works"
        self.session = requests.Session()
        # è®¾ç½®ç”¨æˆ·ä»£ç†é¿å…è¢«é™åˆ¶
        self.session.headers.update({
            'User-Agent': 'Multi-Agent-Debate-Platform/1.0 (mailto:admin@example.com)'
        })
    
    def search(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """æ£€ç´¢æœŸåˆŠæ–‡ç« """
        try:
            params = {
                'query': query,
                'rows': max_results,
                'sort': 'relevance',
                'filter': 'type:journal-article',
                'select': 'title,author,abstract,URL,published-print,container-title'
            }
            
            print(f"ğŸ” æ­£åœ¨CrossRefæ£€ç´¢: {query} (æœ€å¤š{max_results}ç¯‡)")
            response = self.session.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            return self._parse_crossref_response(response.json())
            
        except Exception as e:
            print(f"âŒ CrossRefæ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_crossref_response(self, data: dict) -> List[SearchResult]:
        """è§£æCrossRef APIå“åº”"""
        results = []
        
        try:
            items = data.get('message', {}).get('items', [])
            
            for item in items:
                # æ ‡é¢˜
                title_list = item.get('title', [])
                title = title_list[0] if title_list else "æ— æ ‡é¢˜"
                
                # ä½œè€…
                authors = []
                author_list = item.get('author', [])
                for author in author_list:
                    given = author.get('given', '')
                    family = author.get('family', '')
                    if given and family:
                        authors.append(f"{given} {family}")
                    elif family:
                        authors.append(family)
                
                # æ‘˜è¦ï¼ˆCrossRefé€šå¸¸ä¸æä¾›å®Œæ•´æ‘˜è¦ï¼‰
                abstract = item.get('abstract', 'æ‘˜è¦ä¿¡æ¯éœ€è¦è®¿é—®åŸæ–‡')
                
                # URL
                url = item.get('URL', '')
                
                # å‘å¸ƒæ—¶é—´
                published_date = ""
                date_parts = item.get('published-print', {}).get('date-parts', [])
                if date_parts and len(date_parts[0]) >= 3:
                    year, month, day = date_parts[0][:3]
                    published_date = f"{year}-{month:02d}-{day:02d}"
                elif date_parts and len(date_parts[0]) >= 1:
                    published_date = str(date_parts[0][0])
                
                result = SearchResult(
                    title=title,
                    authors=authors,
                    abstract=abstract,
                    url=url,
                    published_date=published_date,
                    source="CrossRef"
                )
                
                results.append(result)
                
        except Exception as e:
            print(f"âŒ CrossRefå“åº”è§£æé”™è¯¯: {e}")
        
        return results

class RAGEnhancer:
    """RAGå¢å¼ºå™¨ - å¤„ç†æ£€ç´¢ç»“æœå¹¶ç”Ÿæˆæ´å¯Ÿ"""
    
    def __init__(self, llm: ChatDeepSeek):
        self.llm = llm
        self.analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯ç ”ç©¶åˆ†æä¸“å®¶ã€‚åŸºäºç»™å®šçš„å­¦æœ¯è®ºæ–‡ä¿¡æ¯ï¼Œæå–å’Œæ€»ç»“å…³é”®å‘ç°ï¼Œä¸ºç‰¹å®šè§’è‰²çš„è¾©è®ºæä¾›æ”¯æ’‘ã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. åˆ†æè®ºæ–‡çš„æ ¸å¿ƒè§‚ç‚¹å’Œå‘ç°
2. æå–ä¸è¾©è®ºä¸»é¢˜å’ŒæŒ‡å®šè§’è‰²ç›¸å…³çš„å…³é”®è¯æ®
3. ç®€æ´åœ°æ€»ç»“ä¸»è¦è®ºç‚¹ï¼ˆ2-3å¥è¯ï¼‰
4. è¯„ä¼°ç ”ç©¶çš„å¯ä¿¡åº¦å’Œç›¸å…³æ€§

ä¸“å®¶è§’è‰²ï¼š{agent_role}
è®ºæ–‡ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{title}
ä½œè€…ï¼š{authors}
æ‘˜è¦ï¼š{abstract}
å‘å¸ƒæ—¶é—´ï¼š{published_date}
æ¥æºï¼š{source}

è¾©è®ºä¸»é¢˜ï¼š{debate_topic}

è¯·ç‰¹åˆ«å…³æ³¨ä¸{agent_role}ä¸“ä¸šé¢†åŸŸç›¸å…³çš„å†…å®¹ï¼Œæä¾›ï¼š
1. å…³é”®å‘ç°ï¼ˆæ ¸å¿ƒè§‚ç‚¹å’Œè¯æ®ï¼‰
2. ä¸è¾©è®ºä¸»é¢˜çš„ç›¸å…³æ€§è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰
3. å»ºè®®è¯¥è§’è‰²åœ¨è¾©è®ºä¸­å¦‚ä½•å¼•ç”¨è¿™é¡¹ç ”ç©¶"""),
            ("user", "è¯·åˆ†æè¿™ç¯‡è®ºæ–‡å¹¶æä¾›å…³é”®æ´å¯Ÿ")
        ])
    
    def enhance_results(self, results: List[SearchResult], debate_topic: str, agent_role: str = "") -> List[SearchResult]:
        """å¢å¼ºæ£€ç´¢ç»“æœï¼Œæå–å…³é”®æ´å¯Ÿï¼ˆé’ˆå¯¹ç‰¹å®šè§’è‰²ä¼˜åŒ–ï¼‰"""
        enhanced_results = []
        
        for result in results:
            try:
                # ä½¿ç”¨LLMåˆ†æè®ºæ–‡ï¼ˆè€ƒè™‘ä¸“å®¶è§’è‰²ï¼‰
                analysis = self._analyze_paper(result, debate_topic, agent_role)
                result.key_findings = analysis.get('key_findings', '')
                result.relevance_score = analysis.get('relevance_score', 5.0)
                enhanced_results.append(result)
                
                # é¿å…APIé™åˆ¶
                time.sleep(1)
                
            except Exception as e:
                print(f"âŒ è®ºæ–‡åˆ†æå¤±è´¥ {result.title}: {e}")
                # å³ä½¿åˆ†æå¤±è´¥ä¹Ÿä¿ç•™åŸå§‹ç»“æœ
                enhanced_results.append(result)
        
        # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
        enhanced_results.sort(key=lambda x: x.relevance_score, reverse=True)
        return enhanced_results
    
    def _analyze_paper(self, result: SearchResult, debate_topic: str, agent_role: str = "") -> dict:
        """åˆ†æå•ç¯‡è®ºæ–‡ï¼ˆé’ˆå¯¹ç‰¹å®šè§’è‰²ï¼‰"""
        try:
            pipe = self.analysis_prompt | self.llm | StrOutputParser()
            
            response = pipe.invoke({
                'agent_role': agent_role,
                'title': result.title,
                'authors': ', '.join(result.authors[:3]),
                'abstract': result.abstract[:1000],
                'published_date': result.published_date,
                'source': result.source,
                'debate_topic': debate_topic
            })
            
            # ç®€å•è§£æå“åº”
            lines = response.strip().split('\n')
            key_findings = ""
            relevance_score = 5.0
            
            for line in lines:
                if 'å…³é”®å‘ç°' in line or 'æ ¸å¿ƒè§‚ç‚¹' in line:
                    key_findings = line.split('ï¼š', 1)[-1].strip()
                elif 'ç›¸å…³æ€§' in line and 'åˆ†' in line:
                    try:
                        import re
                        score_match = re.search(r'(\d+)', line)
                        if score_match:
                            relevance_score = float(score_match.group(1))
                    except:
                        pass
            
            return {
                'key_findings': key_findings or response[:200],
                'relevance_score': min(max(relevance_score, 1.0), 10.0)
            }
            
        except Exception as e:
            print(f"âŒ LLMåˆ†æé”™è¯¯: {e}")
            return {
                'key_findings': result.abstract[:150] + "...",
                'relevance_score': 5.0
            }

class DynamicRAGModule:
    """åŠ¨æ€RAGä¸»æ¨¡å—ï¼ˆä¿®å¤ç‰ˆï¼Œæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰é…ç½®ï¼‰"""
    
    def __init__(self, llm: ChatDeepSeek):
        self.llm = llm
        self.cache = RAGCache()
        self.arxiv_searcher = ArxivSearcher()
        self.crossref_searcher = CrossRefSearcher()
        self.enhancer = RAGEnhancer(llm)
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨ï¼ˆå¯é€‰ï¼Œç”¨äºæ›´å¤æ‚çš„ç›¸ä¼¼æ€§æ£€ç´¢ï¼‰
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name=RAG_CONFIG["embedding_model"]
            )
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.embeddings = None
    
    def search_academic_sources(self, 
                              topic: str, 
                              sources: List[str] = ["arxiv", "crossref"],
                              max_results_per_source: int = None,
                              agent_role: str = "") -> List[SearchResult]:
        """
        æœç´¢å­¦æœ¯æ•°æ®æºï¼ˆä¿®å¤ç‰ˆï¼Œæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ç»“æœæ•°é‡ï¼‰
        
        Args:
            topic: æœç´¢ä¸»é¢˜
            sources: æ•°æ®æºåˆ—è¡¨
            max_results_per_source: æ¯ä¸ªæ•°æ®æºçš„æœ€å¤§ç»“æœæ•°ï¼ˆç”¨æˆ·å¯é…ç½®ï¼‰
            agent_role: ä¸“å®¶è§’è‰²ï¼ˆç”¨äºå®šåˆ¶åŒ–åˆ†æï¼‰
        """
        
        if max_results_per_source is None:
            max_results_per_source = RAG_CONFIG["max_results_per_source"]
        
        # ğŸ”§ éªŒè¯ç”¨æˆ·é…ç½®
        print(f"ğŸ”§ RAGæ£€ç´¢é…ç½®ï¼šæ¯æºæœ€å¤š{max_results_per_source}ç¯‡ï¼Œè§’è‰²å®šåˆ¶ï¼š{agent_role}")
        
        # æ£€æŸ¥ç¼“å­˜
        cached_results = self.cache.get_cached_results(topic, sources)
        if cached_results:
            print(f"âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ: {len(cached_results)} ç¯‡è®ºæ–‡")
            # å¦‚æœæœ‰è§’è‰²ä¿¡æ¯ï¼Œé‡æ–°æ’åºä»¥é€‚åˆè¯¥è§’è‰²
            if agent_role and self.llm:
                cached_results = self.enhancer.enhance_results(cached_results, topic, agent_role)
            return cached_results
        
        all_results = []
        
        # arXivæ£€ç´¢
        if "arxiv" in sources:
            arxiv_results = self.arxiv_searcher.search(topic, max_results_per_source)
            all_results.extend(arxiv_results)
            print(f"ğŸ“š arXivæ‰¾åˆ° {len(arxiv_results)} ç¯‡è®ºæ–‡ï¼ˆè®¾ç½®ä¸Šé™ï¼š{max_results_per_source}ç¯‡ï¼‰")
        
        # CrossRefæ£€ç´¢
        if "crossref" in sources:
            crossref_results = self.crossref_searcher.search(topic, max_results_per_source)
            all_results.extend(crossref_results)
            print(f"ğŸ“š CrossRefæ‰¾åˆ° {len(crossref_results)} ç¯‡è®ºæ–‡ï¼ˆè®¾ç½®ä¸Šé™ï¼š{max_results_per_source}ç¯‡ï¼‰")
        
        # ä½¿ç”¨LLMå¢å¼ºç»“æœï¼ˆè€ƒè™‘ä¸“å®¶è§’è‰²ï¼‰
        if all_results and self.llm:
            print(f"ğŸ¤– ä½¿ç”¨AIåˆ†æè®ºæ–‡ç›¸å…³æ€§{'ï¼ˆä¸º' + agent_role + 'å®šåˆ¶ï¼‰' if agent_role else ''}...")
            all_results = self.enhancer.enhance_results(all_results, topic, agent_role)
        
        # ç¼“å­˜ç»“æœ
        if all_results:
            self.cache.cache_results(topic, sources, all_results)
        
        return all_results
    
    def get_rag_context_for_agent(self, 
                                 agent_role: str, 
                                 debate_topic: str, 
                                 max_sources: int = 3,
                                 max_results_per_source: int = 2,
                                 force_refresh: bool = False) -> str:
        """
        ä¸ºç‰¹å®šè§’è‰²è·å–RAGä¸Šä¸‹æ–‡ï¼ˆä¿®å¤ç‰ˆï¼Œå®Œå…¨æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰é…ç½®ï¼‰
        
        Args:
            agent_role: ä¸“å®¶è§’è‰²
            debate_topic: è¾©è®ºä¸»é¢˜
            max_sources: æœ€å¤§å‚è€ƒæ–‡çŒ®æ•°ï¼ˆæ¥è‡ªç”¨æˆ·è®¾ç½®ï¼‰- ğŸ”§ å…³é”®ä¿®å¤ç‚¹
            max_results_per_source: æ¯ä¸ªæ•°æ®æºçš„æœ€å¤§æ£€ç´¢æ•°
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
        """
        
        # ğŸ”§ å…³é”®éªŒè¯ï¼šç¡®è®¤æ¥æ”¶åˆ°ç”¨æˆ·è®¾ç½®
        print(f"ğŸ”§ RAGä¸Šä¸‹æ–‡é…ç½®ç¡®è®¤ï¼šä¸“å®¶{agent_role}ï¼Œæœ€å¤§æ–‡çŒ®æ•°{max_sources}ç¯‡ï¼ˆç”¨æˆ·è®¾ç½®ï¼‰ï¼Œæ¯æº{max_results_per_source}ç¯‡")
        
        # å¦‚æœä¸å¼ºåˆ¶åˆ·æ–°ï¼Œå…ˆæ£€æŸ¥ä¸“å®¶ç¼“å­˜
        if not force_refresh:
            cached_context = self.cache.get_agent_cached_context(agent_role, debate_topic)
            if cached_context:
                # ğŸ”§ éªŒè¯ç¼“å­˜ä¸­çš„æ–‡çŒ®æ•°é‡æ˜¯å¦ç¬¦åˆç”¨æˆ·è®¾ç½®
                cached_ref_count = cached_context.count('å‚è€ƒèµ„æ–™')
                print(f"ğŸ“š ä½¿ç”¨ä¸“å®¶ {agent_role} çš„ç¼“å­˜å­¦æœ¯èµ„æ–™ï¼š{cached_ref_count}ç¯‡")
                
                # å¦‚æœç¼“å­˜çš„æ•°é‡ä¸ç¬¦åˆç”¨æˆ·å½“å‰è®¾ç½®ï¼Œé‡æ–°æ£€ç´¢
                if cached_ref_count != max_sources:
                    print(f"ğŸ”„ ç¼“å­˜æ–‡çŒ®æ•°({cached_ref_count})ä¸ç”¨æˆ·è®¾ç½®({max_sources})ä¸ç¬¦ï¼Œé‡æ–°æ£€ç´¢...")
                else:
                    return cached_context
        
        # åŸºäºè§’è‰²è°ƒæ•´æœç´¢æŸ¥è¯¢
        role_focused_query = self._create_role_focused_query(agent_role, debate_topic)
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„æ•°é‡è¿›è¡Œæ£€ç´¢
        results = self.search_academic_sources(
            role_focused_query, 
            max_results_per_source=max_results_per_source,  # æ¯æºæ£€ç´¢æ•°
            agent_role=agent_role
        )
        
        if not results:
            context = "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚"
        else:
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šé€‰æ‹©ç”¨æˆ·è®¾ç½®æ•°é‡çš„æ–‡çŒ®ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
            top_results = results[:max_sources]  # ä½¿ç”¨ç”¨æˆ·è®¾ç½®ï¼
            
            print(f"ğŸ“Š æ£€ç´¢ç»“æœå¤„ç†ï¼šä¸ºä¸“å®¶ {agent_role} å®é™…æ£€ç´¢åˆ° {len(results)} ç¯‡ï¼ŒæŒ‰ç”¨æˆ·è®¾ç½®é€‰æ‹©å‰ {len(top_results)} ç¯‡")
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for i, result in enumerate(top_results, 1):
                context_part = f"""
å‚è€ƒèµ„æ–™ {i}:
æ ‡é¢˜: {result.title}
ä½œè€…: {', '.join(result.authors[:2])}
æ¥æº: {result.source} ({result.published_date})
å…³é”®å‘ç°: {result.key_findings or result.abstract[:200]}
ç›¸å…³æ€§: {result.relevance_score}/10
"""
                context_parts.append(context_part.strip())
            
            context = "\n\n".join(context_parts)
            
            # ğŸ”§ éªŒè¯æœ€ç»ˆç»“æœ
            final_ref_count = context.count('å‚è€ƒèµ„æ–™')
            print(f"âœ… ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼š{final_ref_count}ç¯‡å‚è€ƒæ–‡çŒ®ï¼ˆç”¨æˆ·è®¾ç½®ï¼š{max_sources}ç¯‡ï¼‰")
        
        # ç¼“å­˜ç»“æœ
        if context and context != "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚":
            self.cache.cache_agent_context(agent_role, debate_topic, context)
        
        return context
    
    def _create_role_focused_query(self, agent_role: str, debate_topic: str) -> str:
        """åŸºäºè§’è‰²åˆ›å»ºé’ˆå¯¹æ€§æŸ¥è¯¢"""
        role_keywords = {
            "environmentalist": "environment climate sustainability ecology",
            "economist": "economic cost benefit market analysis",
            "policy_maker": "policy governance regulation implementation",
            "tech_expert": "technology innovation technical feasibility",
            "sociologist": "social impact society community effects",
            "ethicist": "ethics moral responsibility values"
        }
        
        keywords = role_keywords.get(agent_role, "")
        focused_query = f"{debate_topic} {keywords}".strip()
        print(f"ğŸ¯ ä¸º{agent_role}å®šåˆ¶æŸ¥è¯¢ï¼š{focused_query}")
        return focused_query
    
    def preload_agent_contexts(self, agent_roles: List[str], debate_topic: str, max_refs_per_agent: int = 3):
        """
        é¢„åŠ è½½æ‰€æœ‰ä¸“å®¶çš„ä¸Šä¸‹æ–‡ï¼ˆä¿®å¤ç‰ˆï¼Œæ”¯æŒç”¨æˆ·é…ç½®ï¼‰
        
        Args:
            agent_roles: ä¸“å®¶è§’è‰²åˆ—è¡¨
            debate_topic: è¾©è®ºä¸»é¢˜
            max_refs_per_agent: æ¯ä¸ªä¸“å®¶çš„æœ€å¤§å‚è€ƒæ–‡çŒ®æ•°ï¼ˆç”¨æˆ·è®¾ç½®ï¼‰
        """
        print(f"ğŸš€ å¼€å§‹ä¸º {len(agent_roles)} ä½ä¸“å®¶é¢„åŠ è½½å­¦æœ¯èµ„æ–™...")
        print(f"ğŸ“Š ç”¨æˆ·é…ç½®ï¼šæ¯ä¸“å®¶æœ€å¤š {max_refs_per_agent} ç¯‡å‚è€ƒæ–‡çŒ®")
        
        for agent_role in agent_roles:
            try:
                print(f"ğŸ” ä¸ºä¸“å®¶ {agent_role} æ£€ç´¢å­¦æœ¯èµ„æ–™...")
                context = self.get_rag_context_for_agent(
                    agent_role=agent_role,
                    debate_topic=debate_topic,
                    max_sources=max_refs_per_agent,  # ğŸ”§ ä½¿ç”¨ç”¨æˆ·è®¾ç½®
                    max_results_per_source=2,
                    force_refresh=True  # å¼ºåˆ¶åˆ·æ–°ç¡®ä¿æœ€æ–°èµ„æ–™
                )
                
                if context and context != "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚":
                    actual_count = context.count('å‚è€ƒèµ„æ–™')
                    print(f"âœ… ä¸“å®¶ {agent_role} çš„å­¦æœ¯èµ„æ–™å·²å‡†å¤‡å°±ç»ªï¼š{actual_count}ç¯‡ï¼ˆè®¾ç½®ï¼š{max_refs_per_agent}ç¯‡ï¼‰")
                else:
                    print(f"âš ï¸ ä¸“å®¶ {agent_role} æœªæ‰¾åˆ°ç›¸å…³å­¦æœ¯èµ„æ–™")
                
                # é¿å…APIé™åˆ¶
                time.sleep(2)
                
            except Exception as e:
                print(f"âŒ ä¸ºä¸“å®¶ {agent_role} é¢„åŠ è½½èµ„æ–™å¤±è´¥: {e}")
        
        print("âœ… æ‰€æœ‰ä¸“å®¶çš„å­¦æœ¯èµ„æ–™é¢„åŠ è½½å®Œæˆ")
    
    def clear_all_caches(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        try:
            self.cache.clear_agent_cache()
            # æ¸…ç†é€šç”¨ç¼“å­˜
            for filename in os.listdir(self.cache.cache_dir):
                if filename.endswith('.json') and not filename.startswith('agent_'):
                    os.remove(os.path.join(self.cache.cache_dir, filename))
            print("âœ… å·²æ¸…ç†æ‰€æœ‰ç¼“å­˜")
        except Exception as e:
            print(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
    
    def test_user_config_support(self, agent_role: str = "tech_expert", 
                                debate_topic: str = "artificial intelligence education",
                                test_configs: List[int] = [1, 3, 5]):
        """
        æµ‹è¯•ç”¨æˆ·é…ç½®æ”¯æŒï¼ˆéªŒè¯ä¿®å¤æ•ˆæœï¼‰
        
        Args:
            agent_role: æµ‹è¯•ä¸“å®¶è§’è‰²
            debate_topic: æµ‹è¯•è¾©è®ºä¸»é¢˜  
            test_configs: æµ‹è¯•çš„å‚è€ƒæ–‡çŒ®æ•°é‡åˆ—è¡¨
        """
        print("ğŸ§ª å¼€å§‹æµ‹è¯•ç”¨æˆ·RAGé…ç½®æ”¯æŒ...")
        
        for max_refs in test_configs:
            print(f"\nğŸ“‹ æµ‹è¯•é…ç½®ï¼šæ¯ä¸“å®¶{max_refs}ç¯‡å‚è€ƒæ–‡çŒ®")
            
            # æ¸…ç†ç¼“å­˜ç¡®ä¿é‡æ–°æ£€ç´¢
            self.cache.clear_agent_cache(agent_role)
            
            try:
                context = self.get_rag_context_for_agent(
                    agent_role=agent_role,
                    debate_topic=debate_topic,
                    max_sources=max_refs,  # æµ‹è¯•ç”¨æˆ·è®¾ç½®
                    force_refresh=True
                )
                
                if context and context != "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚":
                    actual_count = context.count('å‚è€ƒèµ„æ–™')
                    status = "âœ…" if actual_count == max_refs else "âŒ"
                    print(f"{status} ç»“æœï¼šå®é™…{actual_count}ç¯‡ï¼ŒæœŸæœ›{max_refs}ç¯‡")
                    
                    if actual_count != max_refs:
                        print(f"âš ï¸ é…ç½®ä¸ç”Ÿæ•ˆï¼è¯·æ£€æŸ¥ä»£ç ä¿®å¤")
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ°å­¦æœ¯èµ„æ–™")
                    
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        
        print("\nğŸ‰ ç”¨æˆ·RAGé…ç½®æµ‹è¯•å®Œæˆï¼")

# å…¨å±€RAGå®ä¾‹ï¼ˆå°†åœ¨graph.pyä¸­åˆå§‹åŒ–ï¼‰
rag_module = None

def initialize_rag_module(llm: ChatDeepSeek) -> DynamicRAGModule:
    """åˆå§‹åŒ–RAGæ¨¡å—"""
    global rag_module
    rag_module = DynamicRAGModule(llm)
    return rag_module

def get_rag_module() -> Optional[DynamicRAGModule]:
    """è·å–RAGæ¨¡å—å®ä¾‹"""
    return rag_module

# æµ‹è¯•å‡½æ•°
def test_rag_module():
    """æµ‹è¯•RAGæ¨¡å—åŠŸèƒ½ï¼ˆåŒ…æ‹¬ç”¨æˆ·é…ç½®æ”¯æŒï¼‰"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•ä¿®å¤ç‰ˆRAGæ¨¡å—...")
    
    # åˆ›å»ºæµ‹è¯•LLMï¼ˆéœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥ï¼‰
    try:
        from langchain_deepseek import ChatDeepSeek
        test_llm = ChatDeepSeek(model="deepseek-chat", temperature=0.3)
        
        # åˆå§‹åŒ–RAGæ¨¡å—
        rag = initialize_rag_module(test_llm)
        
        # æµ‹è¯•ä¸“å®¶è§’è‰²æ£€ç´¢
        test_topic = "artificial intelligence employment impact"
        test_roles = ["tech_expert", "economist", "sociologist"]
        
        print("ğŸ” æµ‹è¯•ä¸“å®¶è§’è‰²æ£€ç´¢...")
        for role in test_roles:
            # ğŸ”§ æµ‹è¯•ä¸åŒçš„ç”¨æˆ·é…ç½®
            for max_refs in [1, 3, 5]:
                print(f"\nğŸ“Š æµ‹è¯•ï¼š{role} è·å– {max_refs} ç¯‡æ–‡çŒ®")
                context = rag.get_rag_context_for_agent(
                    agent_role=role, 
                    debate_topic=test_topic,
                    max_sources=max_refs,  # æµ‹è¯•ç”¨æˆ·è®¾ç½®
                    force_refresh=True
                )
                
                if context and context != "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚":
                    actual_count = context.count('å‚è€ƒèµ„æ–™')
                    status = "âœ…" if actual_count == max_refs else "âŒ"
                    print(f"{status} ç»“æœï¼šæœŸæœ›{max_refs}ç¯‡ï¼Œå®é™…{actual_count}ç¯‡")
                else:
                    print("âš ï¸ æœªæ‰¾åˆ°å­¦æœ¯èµ„æ–™")
        
        # ä¸“é—¨çš„ç”¨æˆ·é…ç½®æµ‹è¯•
        print("\nğŸ”§ ä¸“é—¨æµ‹è¯•ç”¨æˆ·é…ç½®æ”¯æŒ...")
        rag.test_user_config_support()
            
    except Exception as e:
        print(f"âŒ RAGæ¨¡å—æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    test_rag_module()