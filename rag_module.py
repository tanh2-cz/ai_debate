"""
åŠ¨æ€RAGæ¨¡å— - åŸºäºKimi APIçš„çœŸå®æ–‡çŒ®æ£€ç´¢
ä½¿ç”¨Kimiçš„å¼ºå¤§èƒ½åŠ›è¿›è¡ŒçœŸå®å­¦æœ¯æ–‡çŒ®æ£€ç´¢å’Œåˆ†æ
é‡ç‚¹ï¼šç¡®ä¿æ‰€æœ‰æ£€ç´¢åˆ°çš„å­¦æœ¯èµ„æ–™éƒ½æ˜¯çœŸå®å­˜åœ¨çš„ï¼Œç»ä¸ç¼–é€ è™šå‡è®ºæ–‡
ä¼˜åŒ–ï¼šæ”¯æŒåŸºäºä¸“å®¶è§’è‰²çš„ç¼“å­˜æœºåˆ¶
å¢å¼ºï¼šæ›´å¥½çš„é”™è¯¯å¤„ç†å’Œå¼‚å¸¸å®‰å…¨æ€§
"""

import os
import asyncio
import aiohttp
import requests
import json
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import hashlib
import time
import re

from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_deepseek import ChatDeepSeek
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# é…ç½®
RAG_CONFIG = {
    "max_results_per_source": 5,
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "similarity_threshold": 0.7,
    "cache_duration_hours": 24,
    "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
    # ä¸“å®¶ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
    "agent_cache_duration_hours": 6,
    # Kimi APIé…ç½®
    "kimi_api_url": "https://api.moonshot.cn/v1/chat/completions",
    "kimi_model": "moonshot-v1-8k",
    "kimi_timeout": 60
}

@dataclass
class SearchResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç±» - ç¡®ä¿çœŸå®æ€§"""
    title: str
    authors: List[str]
    abstract: str
    url: str
    published_date: str
    source: str
    relevance_score: float = 0.0
    key_findings: str = ""
    # æ–°å¢ï¼šçœŸå®æ€§éªŒè¯å­—æ®µ
    is_verified: bool = False
    verification_notes: str = ""

class RAGCache:
    """RAGç»“æœç¼“å­˜ç®¡ç†ï¼ˆæ”¯æŒä¸“å®¶è§’è‰²ç¼“å­˜ï¼‰"""
    
    def __init__(self, cache_dir: str = "./rag_cache"):
        self.cache_dir = cache_dir
        self.agent_cache_dir = os.path.join(cache_dir, "agent_cache")
        
        try:
            os.makedirs(cache_dir, exist_ok=True)
            os.makedirs(self.agent_cache_dir, exist_ok=True)
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ç›®å½•åˆ›å»ºå¤±è´¥: {e}")
    
    def _get_cache_key(self, query: str, sources: List[str]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        try:
            key_string = f"{query}_{'-'.join(sorted(sources))}"
            return hashlib.md5(key_string.encode()).hexdigest()
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜é”®ç”Ÿæˆå¤±è´¥: {e}")
            return f"fallback_{hash(query)}"
    
    def _get_agent_cache_key(self, agent_role: str, debate_topic: str) -> str:
        """ç”Ÿæˆä¸“å®¶è§’è‰²ç‰¹å®šçš„ç¼“å­˜é”®"""
        try:
            key_string = f"agent_{agent_role}_{debate_topic}"
            return hashlib.md5(key_string.encode()).hexdigest()
        except Exception as e:
            print(f"âš ï¸ ä¸“å®¶ç¼“å­˜é”®ç”Ÿæˆå¤±è´¥: {e}")
            return f"agent_fallback_{agent_role}_{hash(debate_topic)}"
    
    def get_cached_results(self, query: str, sources: List[str]) -> Optional[List[SearchResult]]:
        """è·å–ç¼“å­˜çš„æ£€ç´¢ç»“æœ"""
        try:
            cache_key = self._get_cache_key(query, sources)
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
            
            if not os.path.exists(cache_file):
                return None
            
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            cache_time = datetime.fromisoformat(cache_data['timestamp'])
            if datetime.now() - cache_time > timedelta(hours=RAG_CONFIG['cache_duration_hours']):
                os.remove(cache_file)
                return None
            
            # é‡æ„SearchResultå¯¹è±¡
            results = []
            for item in cache_data['results']:
                try:
                    results.append(SearchResult(**item))
                except Exception as e:
                    print(f"âš ï¸ ç¼“å­˜ç»“æœè§£æå¤±è´¥: {e}")
                    continue
            
            return results
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜è¯»å–é”™è¯¯: {e}")
            return None
    
    def cache_results(self, query: str, sources: List[str], results: List[SearchResult]):
        """ç¼“å­˜æ£€ç´¢ç»“æœ"""
        try:
            cache_key = self._get_cache_key(query, sources)
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
            
            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'query': query,
                'sources': sources,
                'results': [result.__dict__ for result in results]
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âŒ ç¼“å­˜å†™å…¥é”™è¯¯: {e}")
    
    def get_agent_cached_context(self, agent_role: str, debate_topic: str) -> Optional[str]:
        """è·å–ä¸“å®¶è§’è‰²ç‰¹å®šçš„ç¼“å­˜ä¸Šä¸‹æ–‡"""
        try:
            cache_key = self._get_agent_cache_key(agent_role, debate_topic)
            cache_file = os.path.join(self.agent_cache_dir, f"{cache_key}.json")
            
            if not os.path.exists(cache_file):
                return None
            
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            cache_time = datetime.fromisoformat(cache_data['timestamp'])
            if datetime.now() - cache_time > timedelta(hours=RAG_CONFIG['agent_cache_duration_hours']):
                os.remove(cache_file)
                return None
            
            return cache_data['context']
            
        except Exception as e:
            print(f"âŒ ä¸“å®¶ç¼“å­˜è¯»å–é”™è¯¯: {e}")
            return None
    
    def cache_agent_context(self, agent_role: str, debate_topic: str, context: str):
        """ç¼“å­˜ä¸“å®¶è§’è‰²ç‰¹å®šçš„ä¸Šä¸‹æ–‡"""
        try:
            cache_key = self._get_agent_cache_key(agent_role, debate_topic)
            cache_file = os.path.join(self.agent_cache_dir, f"{cache_key}.json")
            
            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'agent_role': agent_role,
                'debate_topic': debate_topic,
                'context': context
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
            print(f"âœ… å·²ç¼“å­˜ä¸“å®¶ {agent_role} çš„å­¦æœ¯èµ„æ–™")
                
        except Exception as e:
            print(f"âŒ ä¸“å®¶ç¼“å­˜å†™å…¥é”™è¯¯: {e}")
    
    def clear_agent_cache(self, agent_role: str = None):
        """æ¸…ç†ä¸“å®¶ç¼“å­˜ï¼ˆå¯é€‰æ‹©ç‰¹å®šè§’è‰²ï¼‰"""
        try:
            if agent_role:
                # æ¸…ç†ç‰¹å®šè§’è‰²çš„ç¼“å­˜
                for filename in os.listdir(self.agent_cache_dir):
                    if filename.startswith(f"agent_{agent_role}_"):
                        try:
                            os.remove(os.path.join(self.agent_cache_dir, filename))
                        except Exception as e:
                            print(f"âš ï¸ åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {filename}, {e}")
                print(f"âœ… å·²æ¸…ç†ä¸“å®¶ {agent_role} çš„ç¼“å­˜")
            else:
                # æ¸…ç†æ‰€æœ‰ä¸“å®¶ç¼“å­˜
                for filename in os.listdir(self.agent_cache_dir):
                    try:
                        os.remove(os.path.join(self.agent_cache_dir, filename))
                    except Exception as e:
                        print(f"âš ï¸ åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {filename}, {e}")
                print("âœ… å·²æ¸…ç†æ‰€æœ‰ç¼“å­˜")
        except Exception as e:
            print(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")

class KimiSearcher:
    """åŸºäºKimi APIçš„çœŸå®å­¦æœ¯æ–‡çŒ®æ£€ç´¢å™¨"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv("KIMI_API_KEY")
        self.api_url = RAG_CONFIG["kimi_api_url"]
        self.model = RAG_CONFIG["kimi_model"]
        self.session = requests.Session()
        
        if not self.api_key:
            print("âš ï¸ è­¦å‘Š: KIMI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        else:
            print("âœ… Kimi API åˆå§‹åŒ–æˆåŠŸ")
    
    def search(self, query: str, max_results: int = 5, agent_role: str = "") -> List[SearchResult]:
        """ä½¿ç”¨Kimi APIæ£€ç´¢çœŸå®å­˜åœ¨çš„å­¦æœ¯æ–‡çŒ®"""
        if not self.api_key:
            print("âŒ Kimi API Key æœªé…ç½®")
            return []
        
        try:
            # æ„å»ºæ£€ç´¢æç¤ºè¯ï¼Œå¼ºè°ƒçœŸå®æ€§
            search_prompt = self._build_real_search_prompt(query, max_results, agent_role)
            
            print(f"ğŸ” æ­£åœ¨ä½¿ç”¨Kimiæ£€ç´¢çœŸå®å­¦æœ¯æ–‡çŒ®: {query} (æœ€å¤š{max_results}ç¯‡)")
            
            # è°ƒç”¨Kimi API
            response = self._call_kimi_api(search_prompt)
            
            if response:
                # è§£æå“åº”å¹¶éªŒè¯çœŸå®æ€§
                results = self._parse_and_verify_kimi_response(response, query)
                # è¿‡æ»¤æ‰å¯èƒ½è™šå‡çš„ç»“æœ
                verified_results = self._filter_real_results(results)
                return verified_results
            else:
                return []
                
        except Exception as e:
            print(f"âŒ Kimiæ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _build_real_search_prompt(self, query: str, max_results: int, agent_role: str = "") -> str:
        """æ„å»ºå¼ºè°ƒçœŸå®æ€§çš„Kimiæ£€ç´¢æç¤ºè¯"""
        role_context = ""
        if agent_role:
            role_mapping = {
                "environmentalist": "ç¯ä¿ä¸»ä¹‰è€…ã€ç¯å¢ƒç§‘å­¦ä¸“å®¶",
                "economist": "ç»æµå­¦å®¶ã€å¸‚åœºåˆ†æå¸ˆ",
                "policy_maker": "æ”¿ç­–åˆ¶å®šè€…ã€å…¬å…±ç®¡ç†ä¸“å®¶",
                "tech_expert": "æŠ€æœ¯ä¸“å®¶ã€ç§‘æŠ€ç ”ç©¶è€…",
                "sociologist": "ç¤¾ä¼šå­¦å®¶ã€ç¤¾ä¼šå½±å“ç ”ç©¶ä¸“å®¶",
                "ethicist": "ä¼¦ç†å­¦å®¶ã€é“å¾·å“²å­¦ç ”ç©¶è€…"
            }
            role_context = f"ç‰¹åˆ«å…³æ³¨{role_mapping.get(agent_role, agent_role)}çš„è§†è§’ï¼Œ"
        
        prompt = f"""è¯·ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯ç ”ç©¶åŠ©æ‰‹ï¼Œ{role_context}å¸®æˆ‘æ£€ç´¢å…³äº"{query}"çš„çœŸå®å­˜åœ¨çš„å­¦æœ¯æ–‡çŒ®å’Œç ”ç©¶æˆæœã€‚

ğŸš¨ é‡è¦è¦æ±‚ - ç»å¯¹çœŸå®æ€§ï¼š
1. åªèƒ½æä¾›çœŸå®å­˜åœ¨çš„å­¦æœ¯è®ºæ–‡å’Œç ”ç©¶æŠ¥å‘Š
2. ä¸å¾—ç¼–é€ æˆ–è™šæ„ä»»ä½•è®ºæ–‡ä¿¡æ¯
3. å¦‚æœæ— æ³•ç¡®è®¤è®ºæ–‡çš„çœŸå®æ€§ï¼Œè¯·æ˜ç¡®è¯´æ˜
4. å¦‚æœæ‰¾ä¸åˆ°è¶³å¤Ÿçš„çœŸå®æ–‡çŒ®ï¼Œè¯·è¯šå®å›å¤æ‰¾åˆ°çš„å®é™…æ•°é‡

æ£€ç´¢è¦æ±‚ï¼š
1. å¯»æ‰¾{max_results}ç¯‡çœŸå®çš„é«˜è´¨é‡å­¦æœ¯æ–‡çŒ®æˆ–ç ”ç©¶æŠ¥å‘Š
2. ä¼˜å…ˆé€‰æ‹©è¿‘5å¹´å†…å‘è¡¨çš„æƒå¨è®ºæ–‡
3. åŒ…å«ä¸­è‹±æ–‡æ–‡çŒ®ï¼Œä¼˜å…ˆè€ƒè™‘å½±å“å› å­è¾ƒé«˜çš„æœŸåˆŠ
4. æ¯ç¯‡æ–‡çŒ®éœ€è¦åŒ…å«çœŸå®å¯éªŒè¯çš„ä¿¡æ¯ï¼š
   - çœŸå®çš„è®ºæ–‡æ ‡é¢˜
   - çœŸå®çš„ä½œè€…å§“å
   - çœŸå®çš„å‘è¡¨æ—¶é—´å’ŒæœŸåˆŠ
   - è®ºæ–‡çš„å®é™…æ ¸å¿ƒè§‚ç‚¹
   - çœŸå®å¯è®¿é—®çš„DOIæˆ–é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰ä¿¡æ¯éƒ½æ˜¯çœŸå®çš„ï¼š

```json
[
  {{
    "title": "çœŸå®çš„è®ºæ–‡æ ‡é¢˜ï¼ˆä¸­è‹±æ–‡å‡å¯ï¼‰",
    "authors": ["çœŸå®ä½œè€…1", "çœŸå®ä½œè€…2"],
    "abstract": "è®ºæ–‡çœŸå®æ‘˜è¦æˆ–æ ¸å¿ƒå†…å®¹æ¦‚è¿°",
    "published_date": "çœŸå®å‘è¡¨æ—¥æœŸ(YYYY-MM-DDæ ¼å¼)",
    "key_findings": "è®ºæ–‡çš„å®é™…ä¸»è¦å‘ç°å’Œè§‚ç‚¹",
    "relevance_score": 8.5,
    "source": "çœŸå®çš„æœŸåˆŠåç§°æˆ–å‡ºç‰ˆæœºæ„",
    "url": "çœŸå®çš„DOIé“¾æ¥æˆ–å®˜æ–¹é“¾æ¥",
    "verification_notes": "çœŸå®æ€§è¯´æ˜ï¼Œå¦‚ï¼š'è¯¥è®ºæ–‡å‘è¡¨åœ¨NatureæœŸåˆŠ2023å¹´ç¬¬XXæœŸ'"
  }}
]
```

å…³é”®æé†’ï¼š
- å¦‚æœæ‰¾ä¸åˆ°{max_results}ç¯‡çœŸå®ç›¸å…³æ–‡çŒ®ï¼Œè¯·è¿”å›å®é™…æ‰¾åˆ°çš„æ•°é‡
- æ¯ç¯‡è®ºæ–‡éƒ½å¿…é¡»æ˜¯çœŸå®å­˜åœ¨çš„ï¼Œå¯ä»¥é€šè¿‡å­¦æœ¯æ•°æ®åº“éªŒè¯
- ä¸è¦ä¸ºäº†å‡‘æ•°è€Œç¼–é€ ä»»ä½•è™šå‡ä¿¡æ¯
- å¦‚æœæŸä¸ªä¿¡æ¯ä¸ç¡®å®šï¼Œè¯·æ ‡æ³¨"å¾…ç¡®è®¤"è€Œä¸æ˜¯ç¼–é€ 

ç°åœ¨è¯·ä¸ºæˆ‘æ£€ç´¢å…³äº"{query}"çš„çœŸå®å­¦æœ¯æ–‡çŒ®ï¼š
"""
        return prompt
    
    def _call_kimi_api(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨Kimi API"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": self.model,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 0.1,  # é™ä½æ¸©åº¦ä»¥æé«˜å‡†ç¡®æ€§
                "max_tokens": 4000
            }
            
            response = self.session.post(
                self.api_url,
                headers=headers,
                json=data,
                timeout=RAG_CONFIG["kimi_timeout"]
            )
            
            response.raise_for_status()
            result = response.json()
            
            if "choices" in result and len(result["choices"]) > 0:
                return result["choices"][0]["message"]["content"]
            else:
                print("âŒ Kimi API å“åº”æ ¼å¼å¼‚å¸¸")
                return None
                
        except requests.exceptions.Timeout:
            print("âŒ Kimi API è¯·æ±‚è¶…æ—¶")
            return None
        except requests.exceptions.RequestException as e:
            print(f"âŒ Kimi API è¯·æ±‚é”™è¯¯: {e}")
            return None
        except Exception as e:
            print(f"âŒ Kimi API è°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _parse_and_verify_kimi_response(self, response: str, query: str) -> List[SearchResult]:
        """è§£æKimi APIå“åº”å¹¶åˆæ­¥éªŒè¯çœŸå®æ€§"""
        results = []
        
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            
            if json_start == -1 or json_end == 0:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•è§£ææ–‡æœ¬æ ¼å¼
                return self._parse_text_response(response, query)
            
            json_str = response[json_start:json_end]
            papers = json.loads(json_str)
            
            for paper in papers:
                try:
                    # åŸºæœ¬çœŸå®æ€§æ£€æŸ¥
                    title = paper.get("title", "").strip()
                    authors = paper.get("authors", [])
                    source = paper.get("source", "").strip()
                    
                    # è·³è¿‡æ˜æ˜¾è™šå‡çš„æ¡ç›®
                    if not title or len(title) < 10:
                        print(f"âš ï¸ è·³è¿‡æ ‡é¢˜è¿‡çŸ­æˆ–ç¼ºå¤±çš„æ¡ç›®: {title}")
                        continue
                    
                    if not authors or len(authors) == 0:
                        print(f"âš ï¸ è·³è¿‡ç¼ºå°‘ä½œè€…ä¿¡æ¯çš„æ¡ç›®: {title}")
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„ç¼–é€ ç—•è¿¹
                    if self._is_likely_fabricated(paper):
                        print(f"âš ï¸ è·³è¿‡å¯èƒ½ç¼–é€ çš„æ¡ç›®: {title}")
                        continue
                    
                    result = SearchResult(
                        title=title,
                        authors=authors,
                        abstract=paper.get("abstract", ""),
                        url=paper.get("url", "å¾…æŸ¥è¯¢å­¦æœ¯æ•°æ®åº“"),
                        published_date=paper.get("published_date", ""),
                        source=source,
                        relevance_score=float(paper.get("relevance_score", 7.0)),
                        key_findings=paper.get("key_findings", ""),
                        is_verified=False,  # éœ€è¦è¿›ä¸€æ­¥éªŒè¯
                        verification_notes=paper.get("verification_notes", "")
                    )
                    results.append(result)
                    
                except Exception as e:
                    print(f"âš ï¸ è§£æå•ç¯‡æ–‡çŒ®å¤±è´¥: {e}")
                    continue
            
            print(f"âœ… Kimiæ£€ç´¢è§£æ {len(results)} ç¯‡å¯èƒ½çœŸå®çš„æ–‡çŒ®")
            return results
            
        except json.JSONDecodeError:
            print("âš ï¸ JSONè§£æå¤±è´¥ï¼Œå°è¯•æ–‡æœ¬è§£æ")
            return self._parse_text_response(response, query)
        except Exception as e:
            print(f"âŒ Kimiå“åº”è§£æå¤±è´¥: {e}")
            return []
    
    def _is_likely_fabricated(self, paper: dict) -> bool:
        """æ£€æŸ¥è®ºæ–‡ä¿¡æ¯æ˜¯å¦å¯èƒ½æ˜¯ç¼–é€ çš„"""
        try:
            title = paper.get("title", "").lower()
            authors = paper.get("authors", [])
            source = paper.get("source", "").lower()
            
            # æ£€æŸ¥æ ‡é¢˜ä¸­çš„å¯ç–‘æ¨¡å¼
            suspicious_title_patterns = [
                "example paper", "sample study", "hypothetical research",
                "ç¤ºä¾‹è®ºæ–‡", "æ ·æœ¬ç ”ç©¶", "å‡è®¾ç ”ç©¶", "è™šæ„", "ç¼–é€ "
            ]
            
            for pattern in suspicious_title_patterns:
                if pattern in title:
                    return True
            
            # æ£€æŸ¥ä½œè€…å§“åæ˜¯å¦è¿‡äºç®€å•æˆ–å¯ç–‘
            for author in authors:
                if len(author.strip()) < 3 or author.lower() in ["ä½œè€…1", "author1", "ç ”ç©¶è€…"]:
                    return True
            
            # æ£€æŸ¥æœŸåˆŠåç§°æ˜¯å¦å¯ç–‘
            suspicious_sources = [
                "ç¤ºä¾‹æœŸåˆŠ", "sample journal", "example publication",
                "test journal", "è™šæ„æœŸåˆŠ"
            ]
            
            for sus_source in suspicious_sources:
                if sus_source in source:
                    return True
            
            return False
            
        except Exception as e:
            print(f"âš ï¸ çœŸå®æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _parse_text_response(self, response: str, query: str) -> List[SearchResult]:
        """è§£ææ–‡æœ¬æ ¼å¼çš„å“åº”"""
        results = []
        
        try:
            # ç®€å•çš„æ–‡æœ¬è§£æé€»è¾‘
            lines = response.split('\n')
            current_paper = {}
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                if 'æ ‡é¢˜' in line or 'title' in line.lower():
                    if current_paper and current_paper.get('title'):
                        # ä¿å­˜å‰ä¸€ç¯‡æ–‡çŒ®
                        result = self._create_result_from_dict(current_paper, query)
                        if result and not self._is_likely_fabricated(current_paper):
                            results.append(result)
                    current_paper = {'title': line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()}
                elif 'ä½œè€…' in line or 'author' in line.lower():
                    authors_str = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
                    current_paper['authors'] = [a.strip() for a in authors_str.split(',')]
                elif 'æ‘˜è¦' in line or 'abstract' in line.lower():
                    current_paper['abstract'] = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
                elif 'å‘ç°' in line or 'finding' in line.lower():
                    current_paper['key_findings'] = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
                elif 'æœŸåˆŠ' in line or 'journal' in line.lower():
                    current_paper['source'] = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
            
            # å¤„ç†æœ€åä¸€ç¯‡æ–‡çŒ®
            if current_paper and current_paper.get('title'):
                result = self._create_result_from_dict(current_paper, query)
                if result and not self._is_likely_fabricated(current_paper):
                    results.append(result)
            
            print(f"âœ… æ–‡æœ¬è§£æè·å¾— {len(results)} ç¯‡å¯èƒ½çœŸå®çš„æ–‡çŒ®")
            return results
            
        except Exception as e:
            print(f"âŒ æ–‡æœ¬è§£æå¤±è´¥: {e}")
            return []
    
    def _create_result_from_dict(self, paper_dict: dict, query: str) -> Optional[SearchResult]:
        """ä»å­—å…¸åˆ›å»ºSearchResultå¯¹è±¡"""
        try:
            return SearchResult(
                title=paper_dict.get('title', 'æœªçŸ¥æ ‡é¢˜'),
                authors=paper_dict.get('authors', []),
                abstract=paper_dict.get('abstract', ''),
                url=paper_dict.get('url', 'å¾…æŸ¥è¯¢å­¦æœ¯æ•°æ®åº“'),
                published_date=paper_dict.get('published_date', datetime.now().strftime('%Y-%m-%d')),
                source=paper_dict.get('source', 'Kimiæ£€ç´¢'),
                relevance_score=7.0,
                key_findings=paper_dict.get('key_findings', ''),
                is_verified=False,
                verification_notes=""
            )
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºSearchResultå¤±è´¥: {e}")
            return None
    
    def _filter_real_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """è¿‡æ»¤æ‰å¯èƒ½è™šå‡çš„ç»“æœï¼Œåªä¿ç•™çœ‹èµ·æ¥çœŸå®çš„"""
        filtered_results = []
        
        for result in results:
            # è¿›ä¸€æ­¥çš„çœŸå®æ€§æ£€æŸ¥
            if self._appears_authentic(result):
                result.is_verified = True
                filtered_results.append(result)
            else:
                print(f"âš ï¸ è¿‡æ»¤æ‰å¯èƒ½ä¸çœŸå®çš„æ–‡çŒ®: {result.title[:50]}...")
        
        print(f"ğŸ” çœŸå®æ€§è¿‡æ»¤ï¼šä¿ç•™ {len(filtered_results)}/{len(results)} ç¯‡æ–‡çŒ®")
        return filtered_results
    
    def _appears_authentic(self, result: SearchResult) -> bool:
        """æ£€æŸ¥å•ä¸ªç»“æœæ˜¯å¦çœ‹èµ·æ¥çœŸå®"""
        try:
            # æ£€æŸ¥æ ‡é¢˜é•¿åº¦å’Œå¤æ‚æ€§
            if len(result.title) < 15 or len(result.title) > 200:
                return False
            
            # æ£€æŸ¥ä½œè€…æ•°é‡å’Œæ ¼å¼
            if not result.authors or len(result.authors) == 0:
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ„ä¹‰çš„æ‘˜è¦æˆ–å…³é”®å‘ç°
            if not result.abstract and not result.key_findings:
                return False
            
            # æ£€æŸ¥æ—¥æœŸæ ¼å¼
            if result.published_date:
                try:
                    # ç®€å•çš„æ—¥æœŸæ ¼å¼æ£€æŸ¥
                    if re.match(r'\d{4}-\d{1,2}-\d{1,2}', result.published_date):
                        year = int(result.published_date.split('-')[0])
                        if year < 1950 or year > datetime.now().year:
                            return False
                except:
                    pass
            
            # æ£€æŸ¥æ¥æºæ˜¯å¦åˆç†
            if not result.source or len(result.source.strip()) < 3:
                return False
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ çœŸå®æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return False

class RAGEnhancer:
    """RAGå¢å¼ºå™¨ - å¤„ç†æ£€ç´¢ç»“æœå¹¶ç”Ÿæˆæ´å¯Ÿï¼ˆå¼ºè°ƒçœŸå®æ€§ï¼‰"""
    
    def __init__(self, llm: ChatDeepSeek):
        self.llm = llm
        self.analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯ç ”ç©¶åˆ†æä¸“å®¶ã€‚åŸºäºç»™å®šçš„å­¦æœ¯è®ºæ–‡ä¿¡æ¯ï¼Œæå–å’Œæ€»ç»“å…³é”®å‘ç°ï¼Œä¸ºç‰¹å®šè§’è‰²çš„è¾©è®ºæä¾›æ”¯æ’‘ã€‚

ğŸš¨ é‡è¦æé†’ï¼šä½ åˆ†æçš„è®ºæ–‡ä¿¡æ¯æ¥è‡ªKimi APIæ£€ç´¢ï¼Œè¯·ç¡®ä¿ï¼š
1. åªåŸºäºæä¾›çš„çœŸå®è®ºæ–‡ä¿¡æ¯è¿›è¡Œåˆ†æ
2. ä¸è¦æ·»åŠ ä»»ä½•æœªæä¾›çš„è™šå‡ä¿¡æ¯
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜
4. ä¿æŒåˆ†æçš„å®¢è§‚æ€§å’Œå‡†ç¡®æ€§

ä½ çš„ä»»åŠ¡ï¼š
1. åˆ†æè®ºæ–‡çš„æ ¸å¿ƒè§‚ç‚¹å’Œå‘ç°
2. æå–ä¸è¾©è®ºä¸»é¢˜å’ŒæŒ‡å®šè§’è‰²ç›¸å…³çš„å…³é”®è¯æ®
3. ç®€æ´åœ°æ€»ç»“ä¸»è¦è®ºç‚¹ï¼ˆ2-3å¥è¯ï¼‰
4. è¯„ä¼°ç ”ç©¶çš„å¯ä¿¡åº¦å’Œç›¸å…³æ€§

ä¸“å®¶è§’è‰²ï¼š{agent_role}
è®ºæ–‡ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{title}
ä½œè€…ï¼š{authors}
æ ¸å¿ƒå†…å®¹ï¼š{abstract}
ä¸»è¦å‘ç°ï¼š{key_findings}
å‘å¸ƒæ—¶é—´ï¼š{published_date}
æ¥æºï¼š{source}

è¾©è®ºä¸»é¢˜ï¼š{debate_topic}

è¯·ç‰¹åˆ«å…³æ³¨ä¸{agent_role}ä¸“ä¸šé¢†åŸŸç›¸å…³çš„å†…å®¹ï¼Œæä¾›ï¼š
1. å…³é”®å‘ç°ï¼ˆæ ¸å¿ƒè§‚ç‚¹å’Œè¯æ®ï¼‰
2. ä¸è¾©è®ºä¸»é¢˜çš„ç›¸å…³æ€§è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰
3. å»ºè®®è¯¥è§’è‰²åœ¨è¾©è®ºä¸­å¦‚ä½•å¼•ç”¨è¿™é¡¹ç ”ç©¶"""),
            ("user", "è¯·åŸºäºçœŸå®çš„è®ºæ–‡ä¿¡æ¯åˆ†æå¹¶æä¾›å…³é”®æ´å¯Ÿ")
        ])
    
    def enhance_results(self, results: List[SearchResult], debate_topic: str, agent_role: str = "") -> List[SearchResult]:
        """å¢å¼ºæ£€ç´¢ç»“æœï¼Œæå–å…³é”®æ´å¯Ÿï¼ˆé’ˆå¯¹ç‰¹å®šè§’è‰²ä¼˜åŒ–ï¼‰"""
        enhanced_results = []
        
        for result in results:
            try:
                # å¦‚æœç»“æœå·²ç»æœ‰key_findingsï¼Œç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™ç”¨LLMåˆ†æ
                if not result.key_findings and self.llm:
                    analysis = self._analyze_paper(result, debate_topic, agent_role)
                    result.key_findings = analysis.get('key_findings', result.abstract[:200])
                    result.relevance_score = analysis.get('relevance_score', result.relevance_score)
                
                enhanced_results.append(result)
                
                # é¿å…APIé™åˆ¶
                time.sleep(1)
                
            except Exception as e:
                print(f"âŒ è®ºæ–‡åˆ†æå¤±è´¥ {result.title}: {e}")
                # å³ä½¿åˆ†æå¤±è´¥ä¹Ÿä¿ç•™åŸå§‹ç»“æœ
                if not result.key_findings:
                    result.key_findings = result.abstract[:150] + "..."
                enhanced_results.append(result)
        
        # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
        try:
            enhanced_results.sort(key=lambda x: x.relevance_score, reverse=True)
        except Exception as e:
            print(f"âš ï¸ ç»“æœæ’åºå¤±è´¥: {e}")
        
        return enhanced_results
    
    def _analyze_paper(self, result: SearchResult, debate_topic: str, agent_role: str = "") -> dict:
        """åˆ†æå•ç¯‡è®ºæ–‡ï¼ˆé’ˆå¯¹ç‰¹å®šè§’è‰²ï¼‰"""
        try:
            if not self.llm:
                return {
                    'key_findings': result.abstract[:150] + "...",
                    'relevance_score': result.relevance_score or 5.0
                }
            
            pipe = self.analysis_prompt | self.llm | StrOutputParser()
            
            response = pipe.invoke({
                'agent_role': agent_role,
                'title': result.title,
                'authors': ', '.join(result.authors[:3]),
                'abstract': result.abstract[:1000],
                'key_findings': result.key_findings[:500] if result.key_findings else "",
                'published_date': result.published_date,
                'source': result.source,
                'debate_topic': debate_topic
            })
            
            # ç®€å•è§£æå“åº”
            lines = response.strip().split('\n')
            key_findings = ""
            relevance_score = result.relevance_score or 5.0
            
            for line in lines:
                if 'å…³é”®å‘ç°' in line or 'æ ¸å¿ƒè§‚ç‚¹' in line:
                    key_findings = line.split('ï¼š', 1)[-1].strip()
                elif 'ç›¸å…³æ€§' in line and 'åˆ†' in line:
                    try:
                        import re
                        score_match = re.search(r'(\d+)', line)
                        if score_match:
                            relevance_score = float(score_match.group(1))
                    except:
                        pass
            
            return {
                'key_findings': key_findings or response[:200],
                'relevance_score': min(max(relevance_score, 1.0), 10.0)
            }
            
        except Exception as e:
            print(f"âŒ LLMåˆ†æé”™è¯¯: {e}")
            return {
                'key_findings': result.abstract[:150] + "...",
                'relevance_score': result.relevance_score or 5.0
            }

class DynamicRAGModule:
    """åŠ¨æ€RAGä¸»æ¨¡å—ï¼ˆåŸºäºKimi APIçš„çœŸå®æ–‡çŒ®æ£€ç´¢ï¼‰"""
    
    def __init__(self, llm: ChatDeepSeek):
        self.llm = llm
        self.cache = RAGCache()
        self.kimi_searcher = KimiSearcher()
        self.enhancer = RAGEnhancer(llm) if llm else None
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨ï¼ˆå¯é€‰ï¼Œç”¨äºæ›´å¤æ‚çš„ç›¸ä¼¼æ€§æ£€ç´¢ï¼‰
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name=RAG_CONFIG["embedding_model"]
            )
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.embeddings = None
    
    def search_academic_sources(self, 
                              topic: str, 
                              sources: List[str] = ["kimi"],
                              max_results_per_source: int = None,
                              agent_role: str = "") -> List[SearchResult]:
        """
        æœç´¢çœŸå®çš„å­¦æœ¯æ•°æ®æºï¼ˆåŸºäºKimi APIï¼‰
        
        Args:
            topic: æœç´¢ä¸»é¢˜
            sources: æ•°æ®æºåˆ—è¡¨ï¼ˆç°åœ¨ä¸»è¦æ˜¯"kimi"ï¼‰
            max_results_per_source: æ¯ä¸ªæ•°æ®æºçš„æœ€å¤§ç»“æœæ•°ï¼ˆç”¨æˆ·å¯é…ç½®ï¼‰
            agent_role: ä¸“å®¶è§’è‰²ï¼ˆç”¨äºå®šåˆ¶åŒ–åˆ†æï¼‰
        """
        
        if max_results_per_source is None:
            max_results_per_source = RAG_CONFIG["max_results_per_source"]
        
        print(f"ğŸ” KimiçœŸå®æ–‡çŒ®æ£€ç´¢é…ç½®ï¼šæœ€å¤š{max_results_per_source}ç¯‡ï¼Œè§’è‰²å®šåˆ¶ï¼š{agent_role}")
        
        # å‚æ•°å®‰å…¨æ£€æŸ¥
        if not topic or not topic.strip():
            print("âš ï¸ æœç´¢ä¸»é¢˜ä¸ºç©º")
            return []
        
        if not sources:
            sources = ["kimi"]  # é»˜è®¤ä½¿ç”¨Kimi
        
        # æ£€æŸ¥ç¼“å­˜
        try:
            cached_results = self.cache.get_cached_results(topic, sources)
            if cached_results:
                print(f"âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ: {len(cached_results)} ç¯‡è®ºæ–‡")
                # å¦‚æœæœ‰è§’è‰²ä¿¡æ¯ï¼Œé‡æ–°æ’åºä»¥é€‚åˆè¯¥è§’è‰²
                if agent_role and self.enhancer:
                    try:
                        cached_results = self.enhancer.enhance_results(cached_results, topic, agent_role)
                    except Exception as e:
                        print(f"âš ï¸ ç¼“å­˜ç»“æœå¢å¼ºå¤±è´¥: {e}")
                return cached_results
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ£€æŸ¥å¤±è´¥: {e}")
        
        all_results = []
        
        # Kimiæ£€ç´¢çœŸå®æ–‡çŒ®
        if "kimi" in sources:
            try:
                kimi_results = self.kimi_searcher.search(topic, max_results_per_source, agent_role)
                all_results.extend(kimi_results)
                print(f"ğŸ“š Kimiæ‰¾åˆ° {len(kimi_results)} ç¯‡çœŸå®è®ºæ–‡ï¼ˆè®¾ç½®ä¸Šé™ï¼š{max_results_per_source}ç¯‡ï¼‰")
                
                # ç»Ÿè®¡çœŸå®æ€§éªŒè¯ç»“æœ
                verified_count = sum(1 for r in kimi_results if r.is_verified)
                print(f"âœ… å…¶ä¸­ {verified_count} ç¯‡é€šè¿‡çœŸå®æ€§éªŒè¯")
                
            except Exception as e:
                print(f"âŒ Kimiæ£€ç´¢å‡ºé”™: {e}")
        
        # ä½¿ç”¨LLMå¢å¼ºç»“æœï¼ˆè€ƒè™‘ä¸“å®¶è§’è‰²ï¼‰
        if all_results and self.enhancer:
            try:
                print(f"ğŸ¤– ä½¿ç”¨AIåˆ†æè®ºæ–‡ç›¸å…³æ€§{'ï¼ˆä¸º' + agent_role + 'å®šåˆ¶ï¼‰' if agent_role else ''}...")
                all_results = self.enhancer.enhance_results(all_results, topic, agent_role)
            except Exception as e:
                print(f"âš ï¸ LLMå¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç»“æœ: {e}")
        
        # ç¼“å­˜ç»“æœï¼ˆåªç¼“å­˜é€šè¿‡éªŒè¯çš„çœŸå®ç»“æœï¼‰
        if all_results:
            try:
                verified_results = [r for r in all_results if r.is_verified]
                if verified_results:
                    self.cache.cache_results(topic, sources, verified_results)
                    print(f"ğŸ’¾ ç¼“å­˜äº† {len(verified_results)} ç¯‡ç»è¿‡éªŒè¯çš„çœŸå®æ–‡çŒ®")
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
        
        return all_results
    
    def get_rag_context_for_agent(self, 
                                 agent_role: str, 
                                 debate_topic: str, 
                                 max_sources: int = 3,
                                 max_results_per_source: int = 2,
                                 force_refresh: bool = False) -> str:
        """
        ä¸ºç‰¹å®šè§’è‰²è·å–åŸºäºçœŸå®æ–‡çŒ®çš„RAGä¸Šä¸‹æ–‡
        
        Args:
            agent_role: ä¸“å®¶è§’è‰²
            debate_topic: è¾©è®ºä¸»é¢˜
            max_sources: æœ€å¤§å‚è€ƒæ–‡çŒ®æ•°ï¼ˆæ¥è‡ªç”¨æˆ·è®¾ç½®ï¼‰
            max_results_per_source: æ¯ä¸ªæ•°æ®æºçš„æœ€å¤§æ£€ç´¢æ•°
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
        """
        
        print(f"ğŸ” ä¸ºä¸“å®¶{agent_role}æ£€ç´¢çœŸå®å­¦æœ¯èµ„æ–™ï¼Œæœ€å¤§æ–‡çŒ®æ•°{max_sources}ç¯‡")
        
        # å‚æ•°å®‰å…¨æ£€æŸ¥
        if not agent_role or not debate_topic:
            print("âš ï¸ ä¸“å®¶è§’è‰²æˆ–è¾©è®ºä¸»é¢˜ä¸ºç©º")
            return "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚"
        
        if max_sources <= 0:
            print("âš ï¸ æœ€å¤§æ–‡çŒ®æ•°è®¾ç½®æ— æ•ˆ")
            return "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚"
        
        # å¦‚æœä¸å¼ºåˆ¶åˆ·æ–°ï¼Œå…ˆæ£€æŸ¥ä¸“å®¶ç¼“å­˜
        if not force_refresh:
            try:
                cached_context = self.cache.get_agent_cached_context(agent_role, debate_topic)
                if cached_context:
                    cached_ref_count = cached_context.count('å‚è€ƒèµ„æ–™')
                    print(f"ğŸ“š ä½¿ç”¨ä¸“å®¶ {agent_role} çš„ç¼“å­˜å­¦æœ¯èµ„æ–™ï¼š{cached_ref_count}ç¯‡")
                    
                    # å¦‚æœç¼“å­˜çš„æ•°é‡ä¸ç¬¦åˆç”¨æˆ·å½“å‰è®¾ç½®ï¼Œé‡æ–°æ£€ç´¢
                    if cached_ref_count != max_sources:
                        print(f"ğŸ”„ ç¼“å­˜æ–‡çŒ®æ•°({cached_ref_count})ä¸ç”¨æˆ·è®¾ç½®({max_sources})ä¸ç¬¦ï¼Œé‡æ–°æ£€ç´¢...")
                    else:
                        return cached_context
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜æ£€æŸ¥å¤±è´¥: {e}")
        
        # åŸºäºè§’è‰²è°ƒæ•´æœç´¢æŸ¥è¯¢
        try:
            role_focused_query = self._create_role_focused_query(agent_role, debate_topic)
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ä¸»é¢˜: {e}")
            role_focused_query = debate_topic
        
        # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„æ•°é‡è¿›è¡ŒçœŸå®æ–‡çŒ®æ£€ç´¢
        try:
            results = self.search_academic_sources(
                role_focused_query, 
                sources=["kimi"],  # ä½¿ç”¨Kimiä½œä¸ºæ•°æ®æº
                max_results_per_source=max_sources,  # ç›´æ¥ä½¿ç”¨ç”¨æˆ·è®¾ç½®
                agent_role=agent_role
            )
        except Exception as e:
            print(f"âŒ Kimiå­¦æœ¯æ£€ç´¢å¤±è´¥: {e}")
            return "å­¦æœ¯èµ„æ–™æ£€ç´¢é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œè¯·åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å‘è¡¨è§‚ç‚¹ã€‚"
        
        if not results:
            context = "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚"
        else:
            try:
                # ä¼˜å…ˆä½¿ç”¨é€šè¿‡éªŒè¯çš„çœŸå®æ–‡çŒ®
                verified_results = [r for r in results if r.is_verified]
                if not verified_results:
                    print("âš ï¸ æœªæ‰¾åˆ°é€šè¿‡éªŒè¯çš„çœŸå®æ–‡çŒ®ï¼Œä½¿ç”¨åŸå§‹ç»“æœ")
                    verified_results = results
                
                # é€‰æ‹©ç”¨æˆ·è®¾ç½®æ•°é‡çš„æ–‡çŒ®
                top_results = verified_results[:max_sources]
                
                print(f"ğŸ“Š æ£€ç´¢ç»“æœå¤„ç†ï¼šä¸ºä¸“å®¶ {agent_role} å®é™…æ£€ç´¢åˆ° {len(results)} ç¯‡ï¼Œå…¶ä¸­ {len(verified_results)} ç¯‡é€šè¿‡éªŒè¯ï¼ŒæŒ‰ç”¨æˆ·è®¾ç½®é€‰æ‹©å‰ {len(top_results)} ç¯‡")
                
                # æ„å»ºä¸Šä¸‹æ–‡
                context_parts = []
                for i, result in enumerate(top_results, 1):
                    try:
                        verification_status = "âœ… å·²éªŒè¯" if result.is_verified else "âš ï¸ å¾…éªŒè¯"
                        context_part = f"""
å‚è€ƒèµ„æ–™ {i}: {verification_status}
æ ‡é¢˜: {result.title}
ä½œè€…: {', '.join(result.authors[:2])}
æ¥æº: {result.source} ({result.published_date})
å…³é”®å‘ç°: {result.key_findings or result.abstract[:200]}
ç›¸å…³æ€§: {result.relevance_score}/10
"""
                        if result.verification_notes:
                            context_part += f"éªŒè¯è¯´æ˜: {result.verification_notes}\n"
                            
                        context_parts.append(context_part.strip())
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†ç¬¬{i}ç¯‡æ–‡çŒ®å¤±è´¥: {e}")
                        continue
                
                context = "\n\n".join(context_parts)
                
                # éªŒè¯æœ€ç»ˆç»“æœ
                final_ref_count = context.count('å‚è€ƒèµ„æ–™')
                verified_final_count = context.count('âœ… å·²éªŒè¯')
                print(f"âœ… ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼š{final_ref_count}ç¯‡å‚è€ƒæ–‡çŒ®ï¼ˆå…¶ä¸­{verified_final_count}ç¯‡å·²éªŒè¯çœŸå®æ€§ï¼‰")
                
            except Exception as e:
                print(f"âŒ ä¸Šä¸‹æ–‡æ„å»ºå¤±è´¥: {e}")
                context = "å­¦æœ¯èµ„æ–™å¤„ç†é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œè¯·åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å‘è¡¨è§‚ç‚¹ã€‚"
        
        # ç¼“å­˜ç»“æœ
        if context and context != "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚":
            try:
                self.cache.cache_agent_context(agent_role, debate_topic, context)
            except Exception as e:
                print(f"âš ï¸ ä¸Šä¸‹æ–‡ç¼“å­˜å¤±è´¥: {e}")
        
        return context
    
    def _create_role_focused_query(self, agent_role: str, debate_topic: str) -> str:
        """åŸºäºè§’è‰²åˆ›å»ºé’ˆå¯¹æ€§æŸ¥è¯¢"""
        try:
            role_keywords = {
                "environmentalist": "ç¯å¢ƒä¿æŠ¤ æ°”å€™å˜åŒ– å¯æŒç»­å‘å±• ç”Ÿæ€å½±å“",
                "economist": "ç»æµå½±å“ æˆæœ¬æ•ˆç›Š å¸‚åœºåˆ†æ ç»æµæ”¿ç­–",
                "policy_maker": "æ”¿ç­–åˆ¶å®š ç›‘ç®¡æªæ–½ æ²»ç†æ¡†æ¶ å®æ–½ç­–ç•¥",
                "tech_expert": "æŠ€æœ¯åˆ›æ–° æŠ€æœ¯å¯è¡Œæ€§ æŠ€æœ¯å‘å±• æŠ€æœ¯å½±å“",
                "sociologist": "ç¤¾ä¼šå½±å“ ç¤¾ä¼šå˜åŒ– ç¤¾ç¾¤æ•ˆåº” ç¤¾ä¼šå…¬å¹³",
                "ethicist": "ä¼¦ç†é“å¾· é“å¾·è´£ä»» ä»·å€¼è§‚å¿µ ä¼¦ç†æ¡†æ¶"
            }
            
            keywords = role_keywords.get(agent_role, "")
            focused_query = f"{debate_topic} {keywords}".strip()
            print(f"ğŸ¯ ä¸º{agent_role}å®šåˆ¶KimiæŸ¥è¯¢ï¼š{focused_query}")
            return focused_query
        except Exception as e:
            print(f"âš ï¸ è§’è‰²æŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {e}")
            return debate_topic
    
    def preload_agent_contexts(self, agent_roles: List[str], debate_topic: str, max_refs_per_agent: int = 3):
        """
        é¢„åŠ è½½æ‰€æœ‰ä¸“å®¶çš„çœŸå®å­¦æœ¯ä¸Šä¸‹æ–‡
        
        Args:
            agent_roles: ä¸“å®¶è§’è‰²åˆ—è¡¨
            debate_topic: è¾©è®ºä¸»é¢˜
            max_refs_per_agent: æ¯ä¸ªä¸“å®¶çš„æœ€å¤§å‚è€ƒæ–‡çŒ®æ•°ï¼ˆç”¨æˆ·è®¾ç½®ï¼‰
        """
        
        if not agent_roles or not debate_topic:
            print("âš ï¸ ä¸“å®¶è§’è‰²åˆ—è¡¨æˆ–è¾©è®ºä¸»é¢˜ä¸ºç©º")
            return
        
        print(f"ğŸš€ å¼€å§‹ä¸º {len(agent_roles)} ä½ä¸“å®¶é¢„åŠ è½½KimiçœŸå®å­¦æœ¯èµ„æ–™...")
        print(f"ğŸ“Š ç”¨æˆ·é…ç½®ï¼šæ¯ä¸“å®¶æœ€å¤š {max_refs_per_agent} ç¯‡å‚è€ƒæ–‡çŒ®")
        
        for agent_role in agent_roles:
            try:
                print(f"ğŸ” ä¸ºä¸“å®¶ {agent_role} ä½¿ç”¨Kimiæ£€ç´¢çœŸå®å­¦æœ¯èµ„æ–™...")
                context = self.get_rag_context_for_agent(
                    agent_role=agent_role,
                    debate_topic=debate_topic,
                    max_sources=max_refs_per_agent,  # ä½¿ç”¨ç”¨æˆ·è®¾ç½®
                    max_results_per_source=2,
                    force_refresh=True  # å¼ºåˆ¶åˆ·æ–°ç¡®ä¿æœ€æ–°èµ„æ–™
                )
                
                if context and context != "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚":
                    actual_count = context.count('å‚è€ƒèµ„æ–™')
                    verified_count = context.count('âœ… å·²éªŒè¯')
                    print(f"âœ… ä¸“å®¶ {agent_role} çš„å­¦æœ¯èµ„æ–™å·²å‡†å¤‡å°±ç»ªï¼š{actual_count}ç¯‡ï¼ˆå…¶ä¸­{verified_count}ç¯‡å·²éªŒè¯ï¼‰")
                else:
                    print(f"âš ï¸ ä¸“å®¶ {agent_role} æœªæ‰¾åˆ°ç›¸å…³å­¦æœ¯èµ„æ–™")
                
                # é¿å…APIé™åˆ¶
                time.sleep(3)  # Kimi APIå¯èƒ½éœ€è¦æ›´é•¿çš„é—´éš”
                
            except Exception as e:
                print(f"âŒ ä¸ºä¸“å®¶ {agent_role} é¢„åŠ è½½èµ„æ–™å¤±è´¥: {e}")
                continue
        
        print("âœ… æ‰€æœ‰ä¸“å®¶çš„KimiçœŸå®å­¦æœ¯èµ„æ–™é¢„åŠ è½½å®Œæˆ")
    
    def clear_all_caches(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        try:
            self.cache.clear_agent_cache()
            # æ¸…ç†é€šç”¨ç¼“å­˜
            for filename in os.listdir(self.cache.cache_dir):
                if filename.endswith('.json') and not filename.startswith('agent_'):
                    try:
                        os.remove(os.path.join(self.cache.cache_dir, filename))
                    except Exception as e:
                        print(f"âš ï¸ åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {filename}, {e}")
            print("âœ… å·²æ¸…ç†æ‰€æœ‰ç¼“å­˜")
        except Exception as e:
            print(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
    
    def test_kimi_real_integration(self, 
                                  agent_role: str = "tech_expert", 
                                  debate_topic: str = "äººå·¥æ™ºèƒ½å¯¹æ•™è‚²çš„å½±å“",
                                  test_configs: List[int] = [1, 3, 5]):
        """
        æµ‹è¯•KimiçœŸå®æ–‡çŒ®æ£€ç´¢é›†æˆ
        
        Args:
            agent_role: æµ‹è¯•ä¸“å®¶è§’è‰²
            debate_topic: æµ‹è¯•è¾©è®ºä¸»é¢˜  
            test_configs: æµ‹è¯•çš„å‚è€ƒæ–‡çŒ®æ•°é‡åˆ—è¡¨
        """
        print("ğŸ§ª å¼€å§‹æµ‹è¯•Kimi APIçœŸå®æ–‡çŒ®æ£€ç´¢é›†æˆ...")
        
        for max_refs in test_configs:
            print(f"\nğŸ“‹ æµ‹è¯•é…ç½®ï¼šæ¯ä¸“å®¶{max_refs}ç¯‡å‚è€ƒæ–‡çŒ®")
            
            # æ¸…ç†ç¼“å­˜ç¡®ä¿é‡æ–°æ£€ç´¢
            try:
                self.cache.clear_agent_cache(agent_role)
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
            
            try:
                context = self.get_rag_context_for_agent(
                    agent_role=agent_role,
                    debate_topic=debate_topic,
                    max_sources=max_refs,  # æµ‹è¯•ç”¨æˆ·è®¾ç½®
                    force_refresh=True
                )
                
                if context and context != "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚":
                    actual_count = context.count('å‚è€ƒèµ„æ–™')
                    verified_count = context.count('âœ… å·²éªŒè¯')
                    status = "âœ…" if actual_count == max_refs else "âŒ"
                    print(f"{status} Kimiç»“æœï¼šå®é™…{actual_count}ç¯‡ï¼ŒæœŸæœ›{max_refs}ç¯‡ï¼Œå…¶ä¸­{verified_count}ç¯‡å·²éªŒè¯çœŸå®æ€§")
                    
                    if actual_count != max_refs:
                        print(f"âš ï¸ é…ç½®ä¸ç”Ÿæ•ˆï¼è¯·æ£€æŸ¥ä»£ç ")
                    if verified_count == 0:
                        print(f"âš ï¸ æœªæ‰¾åˆ°é€šè¿‡éªŒè¯çš„çœŸå®æ–‡çŒ®")
                else:
                    print(f"âš ï¸ Kimiæœªæ‰¾åˆ°å­¦æœ¯èµ„æ–™")
                    
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        
        print("\nğŸ‰ Kimi APIçœŸå®æ–‡çŒ®æ£€ç´¢æµ‹è¯•å®Œæˆï¼")

# å…¨å±€RAGå®ä¾‹ï¼ˆå°†åœ¨graph.pyä¸­åˆå§‹åŒ–ï¼‰
rag_module = None

def initialize_rag_module(llm: ChatDeepSeek) -> DynamicRAGModule:
    """åˆå§‹åŒ–RAGæ¨¡å—ï¼ˆåŸºäºKimi APIçš„çœŸå®æ–‡çŒ®æ£€ç´¢ï¼‰"""
    global rag_module
    try:
        rag_module = DynamicRAGModule(llm)
        print("ğŸ” RAGæ¨¡å—å·²åˆå§‹åŒ–ï¼Œä¸“æ³¨äºKimi APIçœŸå®æ–‡çŒ®æ£€ç´¢")
        return rag_module
    except Exception as e:
        print(f"âŒ Kimi RAGæ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

def get_rag_module() -> Optional[DynamicRAGModule]:
    """è·å–RAGæ¨¡å—å®ä¾‹"""
    return rag_module

# æµ‹è¯•å‡½æ•°
def test_rag_module():
    """æµ‹è¯•åŸºäºKimi APIçš„çœŸå®æ–‡çŒ®æ£€ç´¢RAGæ¨¡å—åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•åŸºäºKimi APIçš„çœŸå®æ–‡çŒ®æ£€ç´¢RAGæ¨¡å—...")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("KIMI_API_KEY"):
        print("âŒ è­¦å‘Š: KIMI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼šexport KIMI_API_KEY=your_api_key")
        return
    
    # åˆ›å»ºæµ‹è¯•LLMï¼ˆéœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥ï¼‰
    try:
        from langchain_deepseek import ChatDeepSeek
        test_llm = ChatDeepSeek(model="deepseek-chat", temperature=0.3)
        
        # åˆå§‹åŒ–RAGæ¨¡å—
        rag = initialize_rag_module(test_llm)
        
        if not rag:
            print("âŒ Kimi RAGæ¨¡å—åˆå§‹åŒ–å¤±è´¥")
            return
        
        # æµ‹è¯•ä¸“å®¶è§’è‰²æ£€ç´¢
        test_topic = "äººå·¥æ™ºèƒ½å¯¹å°±ä¸šçš„å½±å“"
        test_roles = ["tech_expert", "economist", "sociologist"]
        
        print("ğŸ” æµ‹è¯•åŸºäºKimiçš„ä¸“å®¶è§’è‰²çœŸå®æ–‡çŒ®æ£€ç´¢...")
        for role in test_roles:
            # æµ‹è¯•ä¸åŒçš„ç”¨æˆ·é…ç½®
            for max_refs in [1, 3]:
                print(f"\nğŸ“Š æµ‹è¯•ï¼š{role} è·å– {max_refs} ç¯‡çœŸå®æ–‡çŒ®")
                try:
                    context = rag.get_rag_context_for_agent(
                        agent_role=role, 
                        debate_topic=test_topic,
                        max_sources=max_refs,  # æµ‹è¯•ç”¨æˆ·è®¾ç½®
                        force_refresh=True
                    )
                    
                    if context and context != "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚":
                        actual_count = context.count('å‚è€ƒèµ„æ–™')
                        verified_count = context.count('âœ… å·²éªŒè¯')
                        status = "âœ…" if actual_count == max_refs else "âŒ"
                        print(f"{status} Kimiç»“æœï¼šæœŸæœ›{max_refs}ç¯‡ï¼Œå®é™…{actual_count}ç¯‡ï¼ŒéªŒè¯{verified_count}ç¯‡")
                        print(f"å‰100å­—ç¬¦ï¼š{context[:100]}...")
                    else:
                        print("âš ï¸ Kimiæœªæ‰¾åˆ°å­¦æœ¯èµ„æ–™")
                except Exception as e:
                    print(f"âŒ æµ‹è¯•å‡ºé”™: {e}")
        
        # ä¸“é—¨çš„KimiçœŸå®æ€§æ£€ç´¢æµ‹è¯•
        print("\nğŸ”§ ä¸“é—¨æµ‹è¯•KimiçœŸå®æ–‡çŒ®æ£€ç´¢...")
        try:
            rag.test_kimi_real_integration()
        except Exception as e:
            print(f"âŒ KimiçœŸå®æ€§æ£€ç´¢æµ‹è¯•å¤±è´¥: {e}")
            
    except Exception as e:
        print(f"âŒ Kimi RAGæ¨¡å—æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    test_rag_module()