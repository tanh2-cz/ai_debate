"""
åŠ¨æ€RAGæ¨¡å— - åŸºäºKimi APIè”ç½‘æœç´¢çš„å­¦æœ¯æ–‡çŒ®æ£€ç´¢
ä½¿ç”¨Kimi APIçš„$web_searchå·¥å…·è¿›è¡Œå®æ—¶è”ç½‘å­¦æœ¯æ–‡çŒ®æ£€ç´¢å’Œåˆ†æ
é›†æˆJSON ModeåŠŸèƒ½ï¼Œè·å¾—ç»“æ„åŒ–çš„æœç´¢ç»“æœ
æ”¯æŒåŸºäºä¸“å®¶è§’è‰²çš„ç¼“å­˜æœºåˆ¶å’Œè”ç½‘æœç´¢
"""

import os
import asyncio
import aiohttp
import requests
import json
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import hashlib
import time
import re

from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_deepseek import ChatDeepSeek
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# é…ç½®
RAG_CONFIG = {
    "max_results_per_source": 5,
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "similarity_threshold": 0.7,
    "cache_duration_hours": 24,
    "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
    # ä¸“å®¶ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
    "agent_cache_duration_hours": 6,
    # Kimi APIé…ç½®ï¼ˆä½¿ç”¨è”ç½‘æœç´¢ï¼‰
    "api_url": "https://api.moonshot.cn/v1/chat/completions",
    "api_model": "moonshot-v1-auto",
    "api_timeout": 60
}

@dataclass
class SearchResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç±»"""
    title: str
    authors: List[str]
    abstract: str
    url: str
    published_date: str
    source: str
    relevance_score: float = 0.0
    key_findings: str = ""

class RAGCache:
    """RAGç»“æœç¼“å­˜ç®¡ç†ï¼ˆæ”¯æŒä¸“å®¶è§’è‰²ç¼“å­˜ï¼‰"""
    
    def __init__(self, cache_dir: str = "./rag_cache"):
        self.cache_dir = cache_dir
        self.agent_cache_dir = os.path.join(cache_dir, "agent_cache")
        
        try:
            os.makedirs(cache_dir, exist_ok=True)
            os.makedirs(self.agent_cache_dir, exist_ok=True)
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ç›®å½•åˆ›å»ºå¤±è´¥: {e}")
    
    def _get_cache_key(self, query: str, sources: List[str]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        try:
            key_string = f"{query}_{'-'.join(sorted(sources))}"
            return hashlib.md5(key_string.encode()).hexdigest()
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜é”®ç”Ÿæˆå¤±è´¥: {e}")
            return f"fallback_{hash(query)}"
    
    def _get_agent_cache_key(self, agent_role: str, debate_topic: str) -> str:
        """ç”Ÿæˆä¸“å®¶è§’è‰²ç‰¹å®šçš„ç¼“å­˜é”®"""
        try:
            key_string = f"agent_{agent_role}_{debate_topic}"
            return hashlib.md5(key_string.encode()).hexdigest()
        except Exception as e:
            print(f"âš ï¸ ä¸“å®¶ç¼“å­˜é”®ç”Ÿæˆå¤±è´¥: {e}")
            return f"agent_fallback_{agent_role}_{hash(debate_topic)}"
    
    def get_cached_results(self, query: str, sources: List[str]) -> Optional[List[SearchResult]]:
        """è·å–ç¼“å­˜çš„æ£€ç´¢ç»“æœ"""
        try:
            cache_key = self._get_cache_key(query, sources)
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
            
            if not os.path.exists(cache_file):
                return None
            
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            cache_time = datetime.fromisoformat(cache_data['timestamp'])
            if datetime.now() - cache_time > timedelta(hours=RAG_CONFIG['cache_duration_hours']):
                os.remove(cache_file)
                return None
            
            # é‡æ„SearchResultå¯¹è±¡
            results = []
            for item in cache_data['results']:
                try:
                    results.append(SearchResult(**item))
                except Exception as e:
                    print(f"âš ï¸ ç¼“å­˜ç»“æœè§£æå¤±è´¥: {e}")
                    continue
            
            return results
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜è¯»å–é”™è¯¯: {e}")
            return None
    
    def cache_results(self, query: str, sources: List[str], results: List[SearchResult]):
        """ç¼“å­˜æ£€ç´¢ç»“æœ"""
        try:
            cache_key = self._get_cache_key(query, sources)
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
            
            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'query': query,
                'sources': sources,
                'results': [result.__dict__ for result in results]
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âŒ ç¼“å­˜å†™å…¥é”™è¯¯: {e}")
    
    def get_agent_cached_context(self, agent_role: str, debate_topic: str) -> Optional[str]:
        """è·å–ä¸“å®¶è§’è‰²ç‰¹å®šçš„ç¼“å­˜ä¸Šä¸‹æ–‡"""
        try:
            cache_key = self._get_agent_cache_key(agent_role, debate_topic)
            cache_file = os.path.join(self.agent_cache_dir, f"{cache_key}.json")
            
            if not os.path.exists(cache_file):
                return None
            
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            cache_time = datetime.fromisoformat(cache_data['timestamp'])
            if datetime.now() - cache_time > timedelta(hours=RAG_CONFIG['agent_cache_duration_hours']):
                os.remove(cache_file)
                return None
            
            return cache_data['context']
            
        except Exception as e:
            print(f"âŒ ä¸“å®¶ç¼“å­˜è¯»å–é”™è¯¯: {e}")
            return None
    
    def cache_agent_context(self, agent_role: str, debate_topic: str, context: str):
        """ç¼“å­˜ä¸“å®¶è§’è‰²ç‰¹å®šçš„ä¸Šä¸‹æ–‡"""
        try:
            cache_key = self._get_agent_cache_key(agent_role, debate_topic)
            cache_file = os.path.join(self.agent_cache_dir, f"{cache_key}.json")
            
            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'agent_role': agent_role,
                'debate_topic': debate_topic,
                'context': context
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
            print(f"âœ… å·²ç¼“å­˜ä¸“å®¶ {agent_role} çš„å­¦æœ¯èµ„æ–™")
                
        except Exception as e:
            print(f"âŒ ä¸“å®¶ç¼“å­˜å†™å…¥é”™è¯¯: {e}")
    
    def clear_agent_cache(self, agent_role: str = None):
        """æ¸…ç†ä¸“å®¶ç¼“å­˜ï¼ˆå¯é€‰æ‹©ç‰¹å®šè§’è‰²ï¼‰"""
        try:
            if agent_role:
                # æ¸…ç†ç‰¹å®šè§’è‰²çš„ç¼“å­˜
                for filename in os.listdir(self.agent_cache_dir):
                    if filename.startswith(f"agent_{agent_role}_"):
                        try:
                            os.remove(os.path.join(self.agent_cache_dir, filename))
                        except Exception as e:
                            print(f"âš ï¸ åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {filename}, {e}")
                print(f"âœ… å·²æ¸…ç†ä¸“å®¶ {agent_role} çš„ç¼“å­˜")
            else:
                # æ¸…ç†æ‰€æœ‰ä¸“å®¶ç¼“å­˜
                for filename in os.listdir(self.agent_cache_dir):
                    try:
                        os.remove(os.path.join(self.agent_cache_dir, filename))
                    except Exception as e:
                        print(f"âš ï¸ åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {filename}, {e}")
                print("âœ… å·²æ¸…ç†æ‰€æœ‰ç¼“å­˜")
        except Exception as e:
            print(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")

class WebSearchTool:
    """åŸºäºKimi APIçš„$web_searchå·¥å…·å®ç° (é›†æˆJSON Mode)"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv("KIMI_API_KEY")
        self.api_url = RAG_CONFIG["api_url"]
        self.model = RAG_CONFIG["api_model"]
        self.session = requests.Session()
        
        if not self.api_key:
            print("âš ï¸ è­¦å‘Š: KIMI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        else:
            print("âœ… Kimiè”ç½‘æœç´¢å·¥å…·åˆå§‹åŒ–æˆåŠŸ")
    
    def web_search_impl(self, arguments: Dict[str, Any]) -> Any:
        """å®ç°web_searchå·¥å…·çš„å…·ä½“é€»è¾‘"""
        # åœ¨ä½¿ç”¨Kimiæä¾›çš„$web_searchå·¥å…·çš„åœºåˆï¼Œåªéœ€è¦åŸå°ä¸åŠ¨è¿”å›argumentså³å¯
        # ä¸éœ€è¦é¢å¤–çš„å¤„ç†é€»è¾‘
        return arguments
    
    def search_with_web_tool(self, query: str, agent_role: str = "") -> str:
        """ä½¿ç”¨Kimiçš„$web_searchå·¥å…·è¿›è¡Œè”ç½‘æœç´¢ (å¯ç”¨JSON Mode)"""
        if not self.api_key:
            print("âŒ Kimi API Key æœªé…ç½®")
            return "è”ç½‘æœç´¢åŠŸèƒ½ä¸å¯ç”¨ï¼ŒAPIå¯†é’¥æœªè®¾ç½®ã€‚"
        
        try:
            # æ„å»ºæœç´¢æç¤ºè¯ (JSON Mode)
            search_prompt = self._build_web_search_prompt_json(query, agent_role)
            
            print(f"ğŸ” æ­£åœ¨ä½¿ç”¨Kimiè”ç½‘æœç´¢ (JSON Mode): {query}")
            
            # è°ƒç”¨Kimi API with $web_search tool and JSON Mode
            response = self._call_kimi_with_web_search_json(search_prompt)
            
            if response:
                return response
            else:
                return "è”ç½‘æœç´¢æœªè¿”å›æœ‰æ•ˆç»“æœã€‚"
                
        except Exception as e:
            print(f"âŒ è”ç½‘æœç´¢å¤±è´¥: {e}")
            return f"è”ç½‘æœç´¢é‡åˆ°æŠ€æœ¯é—®é¢˜: {str(e)}"
    
    def _build_web_search_prompt_json(self, query: str, agent_role: str = "") -> str:
        """æ„å»ºä½¿ç”¨JSON Modeçš„è”ç½‘æœç´¢æç¤ºè¯"""
        role_context = ""
        if agent_role:
            role_mapping = {
                "environmentalist": "ç¯ä¿ä¸»ä¹‰è€…ã€ç¯å¢ƒç§‘å­¦ä¸“å®¶",
                "economist": "ç»æµå­¦å®¶ã€å¸‚åœºåˆ†æå¸ˆ",
                "policy_maker": "æ”¿ç­–åˆ¶å®šè€…ã€å…¬å…±ç®¡ç†ä¸“å®¶",
                "tech_expert": "æŠ€æœ¯ä¸“å®¶ã€ç§‘æŠ€ç ”ç©¶è€…",
                "sociologist": "ç¤¾ä¼šå­¦å®¶ã€ç¤¾ä¼šå½±å“ç ”ç©¶ä¸“å®¶",
                "ethicist": "ä¼¦ç†å­¦å®¶ã€é“å¾·å“²å­¦ç ”ç©¶è€…"
            }
            role_context = f"ç‰¹åˆ«å…³æ³¨{role_mapping.get(agent_role, agent_role)}çš„è§†è§’ï¼Œ"
        
        prompt = f"""è¯·ä½¿ç”¨è”ç½‘æœç´¢åŠŸèƒ½ï¼Œ{role_context}å¸®æˆ‘æœç´¢å…³äº"{query}"çš„æœ€æ–°ä¿¡æ¯å’Œå­¦æœ¯èµ„æ–™ã€‚

è¯·ä½¿ç”¨å¦‚ä¸‹JSONæ ¼å¼è¾“å‡ºæœç´¢ç»“æœï¼š

{{
  "search_results": [
    {{
      "title": "èµ„æ–™æ ‡é¢˜",
      "source": "æ¥æºç½‘ç«™æˆ–æœŸåˆŠ",
      "published_date": "å‘å¸ƒæ—¶é—´",
      "key_findings": "æ ¸å¿ƒè§‚ç‚¹å’Œå‘ç°",
      "relevance_score": 8,
      "url": "é“¾æ¥åœ°å€"
    }}
  ]
}}

æœç´¢è¦æ±‚ï¼š
1. å¯»æ‰¾æƒå¨çš„å­¦æœ¯æ–‡çŒ®ã€ç ”ç©¶æŠ¥å‘Šå’Œæœ€æ–°èµ„è®¯
2. ä¼˜å…ˆé€‰æ‹©è¿‘æœŸå‘è¡¨çš„é«˜è´¨é‡å†…å®¹
3. åŒ…å«ä¸­è‹±æ–‡èµ„æºï¼Œå…³æ³¨å­¦æœ¯æœŸåˆŠå’Œç ”ç©¶æœºæ„
4. æå–å…³é”®ä¿¡æ¯å¹¶æ•´ç†ä¸ºä¸Šè¿°JSONæ ¼å¼
5. relevance_scoreä¸º1-10çš„ç›¸å…³æ€§è¯„åˆ†

ç°åœ¨è¯·ä¸ºæˆ‘æœç´¢å…³äº"{query}"çš„ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š"""
        
        return prompt
    
    def _call_kimi_with_web_search_json(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨Kimi APIå¹¶æ”¯æŒ$web_searchå·¥å…·å’ŒJSON Mode"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            # åˆå§‹è¯·æ±‚æ¶ˆæ¯
            messages = [
                {"role": "system", "content": "ä½ æ˜¯Kimiã€‚ç”±Moonshot AIæä¾›çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ æ›´æ“…é•¿ä¸­æ–‡å’Œè‹±æ–‡çš„å¯¹è¯ã€‚ä½ ä¼šä¸ºç”¨æˆ·æä¾›å®‰å…¨ï¼Œæœ‰å¸®åŠ©ï¼Œå‡†ç¡®çš„å›ç­”ã€‚åŒæ—¶ï¼Œä½ ä¼šå¯»æ±‚å›åº”ç”¨æˆ·ä½¿ç”¨JSONæ ¼å¼çš„è¦æ±‚ï¼Œå¦‚ç”¨æˆ·è¦æ±‚JSONæ ¼å¼è¾“å‡ºã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            # å¾ªç¯å¤„ç†å¯èƒ½çš„å·¥å…·è°ƒç”¨
            finish_reason = None
            while finish_reason is None or finish_reason == "tool_calls":
                data = {
                    "model": self.model,
                    "messages": messages,
                    "temperature": 0.3,
                    "response_format": {"type": "json_object"},  # å¯ç”¨JSON Mode
                    "tools": [
                        {
                            "type": "builtin_function",
                            "function": {
                                "name": "$web_search",
                            },
                        },
                    ]
                }
                
                response = self.session.post(
                    self.api_url,
                    headers=headers,
                    json=data,
                    timeout=RAG_CONFIG["api_timeout"]
                )
                
                response.raise_for_status()
                result = response.json()
                
                if "choices" not in result or len(result["choices"]) == 0:
                    print("âŒ Kimi API å“åº”æ ¼å¼å¼‚å¸¸")
                    return None
                
                choice = result["choices"][0]
                finish_reason = choice["finish_reason"]  # ä¿®å¤ï¼šä½¿ç”¨å­—å…¸è®¿é—®æ–¹å¼
                
                # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°æ¶ˆæ¯å†å²
                messages.append(choice["message"])  # ä¿®å¤ï¼šä½¿ç”¨å­—å…¸è®¿é—®æ–¹å¼
                
                # åˆ¤æ–­å½“å‰è¿”å›å†…å®¹æ˜¯å¦åŒ…å«tool_calls
                if finish_reason == "tool_calls":
                    # å¤„ç†å·¥å…·è°ƒç”¨
                    tool_calls = choice["message"].get("tool_calls", [])
                    for tool_call in tool_calls:
                        tool_call_name = tool_call["function"]["name"]
                        tool_call_arguments = json.loads(tool_call["function"]["arguments"])
                        
                        if tool_call_name == "$web_search":
                            # ä½¿ç”¨è”ç½‘æœç´¢è¿‡ç¨‹ä¸­ï¼Œç”±Kimiå¤§æ¨¡å‹æ ¹æ®è”ç½‘æœç´¢ç»“æœç”Ÿæˆçš„Tokensæ‰“å°å‡ºæ¥
                            search_content_total_tokens = tool_call_arguments.get("usage", {}).get("total_tokens", 0)
                            print(f"search_content_total_tokens: {search_content_total_tokens}")
                            
                            tool_result = self.web_search_impl(tool_call_arguments)
                        else:
                            tool_result = f"Error: unable to find tool by name '{tool_call_name}'"
                        
                        # ä½¿ç”¨å‡½æ•°æ‰§è¡Œç»“æœæ„é€ ä¸€ä¸ª role=tool çš„ messageï¼Œä»¥æ­¤æ¥å‘æ¨¡å‹å±•ç¤ºå·¥å…·è°ƒç”¨çš„ç»“æœ
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call["id"],
                            "name": tool_call_name,
                            "content": json.dumps(tool_result),
                        })
                
                # å¦‚æœfinish_reasonä¸æ˜¯tool_callsï¼Œè¯´æ˜æ¨¡å‹å·²å®Œæˆå“åº”
                if finish_reason != "tool_calls":
                    return choice["message"]["content"]  # ä¿®å¤ï¼šä½¿ç”¨å­—å…¸è®¿é—®æ–¹å¼
            
            return None
                
        except requests.exceptions.Timeout:
            print("âŒ Kimi API è¯·æ±‚è¶…æ—¶")
            return None
        except requests.exceptions.RequestException as e:
            print(f"âŒ Kimi API è¯·æ±‚é”™è¯¯: {e}")
            return None
        except Exception as e:
            print(f"âŒ Kimi API è°ƒç”¨å¤±è´¥: {e}")
            return None

class AcademicSearcher:
    """åŸºäºKimi APIè”ç½‘æœç´¢çš„å­¦æœ¯æ–‡çŒ®æ£€ç´¢å™¨ (é›†æˆJSON Mode)"""
    
    def __init__(self, api_key: str = None):
        self.web_tool = WebSearchTool(api_key)
        
        if not self.web_tool.api_key:
            print("âš ï¸ è­¦å‘Š: KIMI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        else:
            print("âœ… å­¦æœ¯è”ç½‘æœç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    def search(self, query: str, max_results: int = 5, agent_role: str = "") -> List[SearchResult]:
        """ä½¿ç”¨Kimiè”ç½‘æœç´¢è¿›è¡Œå­¦æœ¯æ–‡çŒ®æ£€ç´¢ (JSON Mode)"""
        try:
            print(f"ğŸ” æ­£åœ¨ä½¿ç”¨Kimiè”ç½‘æœç´¢å­¦æœ¯æ–‡çŒ® (JSON Mode): {query} (æœ€å¤š{max_results}ç¯‡)")
            
            # ä½¿ç”¨è”ç½‘æœç´¢ (JSON Mode)
            search_response = self.web_tool.search_with_web_tool(query, agent_role)
            
            if search_response and search_response != "è”ç½‘æœç´¢æœªè¿”å›æœ‰æ•ˆç»“æœã€‚":
                # è§£æJSONå“åº”
                results = self._parse_json_search_response(search_response, query, max_results)
                return results
            else:
                print("âš ï¸ è”ç½‘æœç´¢æœªè¿”å›æœ‰æ•ˆç»“æœ")
                return []
                
        except Exception as e:
            print(f"âŒ å­¦æœ¯è”ç½‘æœç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_json_search_response(self, response: str, query: str, max_results: int) -> List[SearchResult]:
        """è§£æJSONæ ¼å¼çš„è”ç½‘æœç´¢å“åº”"""
        results = []
        
        try:
            # è§£æJSONå“åº”
            json_data = json.loads(response)
            
            # è·å–æœç´¢ç»“æœæ•°ç»„
            search_results = json_data.get("search_results", [])
            
            for item in search_results[:max_results]:
                try:
                    # æå–ä¿¡æ¯
                    title = item.get("title", "").strip()
                    source = item.get("source", "").strip()
                    published_date = item.get("published_date", "").strip()
                    key_findings = item.get("key_findings", "").strip()
                    url = item.get("url", "").strip()
                    relevance_score = float(item.get("relevance_score", 7.0))
                    
                    # åŸºæœ¬éªŒè¯
                    if title and len(title) > 5:
                        result = SearchResult(
                            title=title,
                            authors=["è”ç½‘æœç´¢è·å–"],
                            abstract=key_findings[:300] if key_findings else "é€šè¿‡è”ç½‘æœç´¢è·å¾—çš„èµ„æ–™",
                            url=url or "é€šè¿‡è”ç½‘æœç´¢è·å¾—",
                            published_date=published_date or datetime.now().strftime('%Y-%m-%d'),
                            source=source or "è”ç½‘æœç´¢",
                            relevance_score=min(max(relevance_score, 1.0), 10.0),
                            key_findings=key_findings
                        )
                        results.append(result)
                        
                except Exception as e:
                    print(f"âš ï¸ è§£æå•ä¸ªJSONæœç´¢ç»“æœå¤±è´¥: {e}")
                    continue
            
            print(f"âœ… JSONè”ç½‘æœç´¢è§£æå¾—åˆ° {len(results)} ç¯‡æ–‡çŒ®")
            return results
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œå°è¯•æ–‡æœ¬è§£æ: {e}")
            return self._fallback_text_extraction(response, query, max_results)
        except Exception as e:
            print(f"âŒ JSONæœç´¢å“åº”è§£æå¤±è´¥: {e}")
            return self._fallback_text_extraction(response, query, max_results)
    
    def _fallback_text_extraction(self, response: str, query: str, max_results: int) -> List[SearchResult]:
        """å¤‡ç”¨æ–‡æœ¬æå–æ–¹æ³•"""
        results = []
        
        try:
            # ç®€å•çš„æ–‡æœ¬åˆ†æ®µå’Œä¿¡æ¯æå–
            sentences = response.split('ã€‚')
            current_title = ""
            current_content = ""
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                # ç®€å•çš„æ ‡é¢˜è¯†åˆ«
                if len(sentence) < 100 and ('ç ”ç©¶' in sentence or 'è®ºæ–‡' in sentence or 'æŠ¥å‘Š' in sentence):
                    if current_title and current_content:
                        # ä¿å­˜å‰ä¸€ä¸ªç»“æœ
                        result = SearchResult(
                            title=current_title,
                            authors=["è”ç½‘æœç´¢è·å–"],
                            abstract=current_content[:200],
                            url="é€šè¿‡è”ç½‘æœç´¢è·å¾—",
                            published_date=datetime.now().strftime('%Y-%m-%d'),
                            source="è”ç½‘æœç´¢",
                            relevance_score=6.0,
                            key_findings=current_content[:150]
                        )
                        results.append(result)
                        
                        if len(results) >= max_results:
                            break
                    
                    current_title = sentence
                    current_content = ""
                else:
                    current_content += sentence + "ã€‚"
            
            # å¤„ç†æœ€åä¸€ä¸ª
            if current_title and current_content and len(results) < max_results:
                result = SearchResult(
                    title=current_title,
                    authors=["è”ç½‘æœç´¢è·å–"],
                    abstract=current_content[:200],
                    url="é€šè¿‡è”ç½‘æœç´¢è·å¾—",
                    published_date=datetime.now().strftime('%Y-%m-%d'),
                    source="è”ç½‘æœç´¢",
                    relevance_score=6.0,
                    key_findings=current_content[:150]
                )
                results.append(result)
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«åŸå§‹å“åº”çš„ç»“æœ
            if not results:
                result = SearchResult(
                    title=f"å…³äº{query}çš„è”ç½‘æœç´¢ç»“æœ",
                    authors=["è”ç½‘æœç´¢è·å–"],
                    abstract=response[:300],
                    url="é€šè¿‡è”ç½‘æœç´¢è·å¾—",
                    published_date=datetime.now().strftime('%Y-%m-%d'),
                    source="è”ç½‘æœç´¢",
                    relevance_score=5.0,
                    key_findings=response[:200]
                )
                results.append(result)
            
            return results
            
        except Exception as e:
            print(f"âŒ å¤‡ç”¨æ–‡æœ¬æå–å¤±è´¥: {e}")
            return []

class RAGEnhancer:
    """RAGå¢å¼ºå™¨ - å¤„ç†æ£€ç´¢ç»“æœå¹¶ç”Ÿæˆæ´å¯Ÿ"""
    
    def __init__(self, llm: ChatDeepSeek):
        self.llm = llm
        self.analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯ç ”ç©¶åˆ†æä¸“å®¶ã€‚åŸºäºç»™å®šçš„è”ç½‘æœç´¢ç»“æœï¼Œæå–å’Œæ€»ç»“å…³é”®å‘ç°ï¼Œä¸ºç‰¹å®šè§’è‰²çš„è¾©è®ºæä¾›æ”¯æ’‘ã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. åˆ†ææœç´¢ç»“æœçš„æ ¸å¿ƒè§‚ç‚¹å’Œå‘ç°
2. æå–ä¸è¾©è®ºä¸»é¢˜å’ŒæŒ‡å®šè§’è‰²ç›¸å…³çš„å…³é”®è¯æ®
3. ç®€æ´åœ°æ€»ç»“ä¸»è¦è®ºç‚¹ï¼ˆ2-3å¥è¯ï¼‰
4. è¯„ä¼°ä¿¡æ¯çš„å¯ä¿¡åº¦å’Œç›¸å…³æ€§

ä¸“å®¶è§’è‰²ï¼š{agent_role}
æœç´¢ç»“æœä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{title}
æ¥æºï¼š{source}
æ ¸å¿ƒå†…å®¹ï¼š{abstract}
ä¸»è¦å‘ç°ï¼š{key_findings}
å‘å¸ƒæ—¶é—´ï¼š{published_date}

è¾©è®ºä¸»é¢˜ï¼š{debate_topic}

è¯·ç‰¹åˆ«å…³æ³¨ä¸{agent_role}ä¸“ä¸šé¢†åŸŸç›¸å…³çš„å†…å®¹ï¼Œæä¾›ï¼š
1. å…³é”®å‘ç°ï¼ˆæ ¸å¿ƒè§‚ç‚¹å’Œè¯æ®ï¼‰
2. ä¸è¾©è®ºä¸»é¢˜çš„ç›¸å…³æ€§è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰
3. å»ºè®®è¯¥è§’è‰²åœ¨è¾©è®ºä¸­å¦‚ä½•å¼•ç”¨è¿™é¡¹ä¿¡æ¯"""),
            ("user", "è¯·åŸºäºæœç´¢ç»“æœåˆ†æå¹¶æä¾›å…³é”®æ´å¯Ÿ")
        ])
    
    def enhance_results(self, results: List[SearchResult], debate_topic: str, agent_role: str = "") -> List[SearchResult]:
        """å¢å¼ºæ£€ç´¢ç»“æœï¼Œæå–å…³é”®æ´å¯Ÿï¼ˆé’ˆå¯¹ç‰¹å®šè§’è‰²ä¼˜åŒ–ï¼‰"""
        enhanced_results = []
        
        for result in results:
            try:
                # å¦‚æœç»“æœå·²ç»æœ‰key_findingsï¼Œå¯ä»¥ç”¨LLMè¿›ä¸€æ­¥åˆ†æ
                if self.llm and result.key_findings:
                    analysis = self._analyze_result(result, debate_topic, agent_role)
                    if analysis.get('enhanced_findings'):
                        result.key_findings = analysis['enhanced_findings']
                    if analysis.get('relevance_score'):
                        result.relevance_score = analysis['relevance_score']
                
                enhanced_results.append(result)
                
                # é¿å…APIé™åˆ¶
                time.sleep(1)
                
            except Exception as e:
                print(f"âŒ æœç´¢ç»“æœåˆ†æå¤±è´¥ {result.title}: {e}")
                # å³ä½¿åˆ†æå¤±è´¥ä¹Ÿä¿ç•™åŸå§‹ç»“æœ
                enhanced_results.append(result)
        
        # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
        try:
            enhanced_results.sort(key=lambda x: x.relevance_score, reverse=True)
        except Exception as e:
            print(f"âš ï¸ ç»“æœæ’åºå¤±è´¥: {e}")
        
        return enhanced_results
    
    def _analyze_result(self, result: SearchResult, debate_topic: str, agent_role: str = "") -> dict:
        """åˆ†æå•ä¸ªæœç´¢ç»“æœï¼ˆé’ˆå¯¹ç‰¹å®šè§’è‰²ï¼‰"""
        try:
            if not self.llm:
                return {
                    'enhanced_findings': result.key_findings,
                    'relevance_score': result.relevance_score or 5.0
                }
            
            pipe = self.analysis_prompt | self.llm | StrOutputParser()
            
            response = pipe.invoke({
                'agent_role': agent_role,
                'title': result.title,
                'source': result.source,
                'abstract': result.abstract[:1000],
                'key_findings': result.key_findings[:500] if result.key_findings else "",
                'published_date': result.published_date,
                'debate_topic': debate_topic
            })
            
            # ç®€å•è§£æå“åº”
            lines = response.strip().split('\n')
            enhanced_findings = ""
            relevance_score = result.relevance_score or 5.0
            
            for line in lines:
                if 'å…³é”®å‘ç°' in line or 'æ ¸å¿ƒè§‚ç‚¹' in line:
                    enhanced_findings = line.split('ï¼š', 1)[-1].strip()
                elif 'ç›¸å…³æ€§' in line and 'åˆ†' in line:
                    try:
                        import re
                        score_match = re.search(r'(\d+)', line)
                        if score_match:
                            relevance_score = float(score_match.group(1))
                    except:
                        pass
            
            return {
                'enhanced_findings': enhanced_findings or response[:200],
                'relevance_score': min(max(relevance_score, 1.0), 10.0)
            }
            
        except Exception as e:
            print(f"âŒ LLMåˆ†æé”™è¯¯: {e}")
            return {
                'enhanced_findings': result.key_findings,
                'relevance_score': result.relevance_score or 5.0
            }

class DynamicRAGModule:
    """åŠ¨æ€RAGä¸»æ¨¡å—ï¼ˆåŸºäºKimi APIè”ç½‘æœç´¢çš„å­¦æœ¯æ–‡çŒ®æ£€ç´¢ï¼Œé›†æˆJSON Modeï¼‰"""
    
    def __init__(self, llm: ChatDeepSeek):
        self.llm = llm
        self.cache = RAGCache()
        self.academic_searcher = AcademicSearcher()
        self.enhancer = RAGEnhancer(llm) if llm else None
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨ï¼ˆå¯é€‰ï¼Œç”¨äºæ›´å¤æ‚çš„ç›¸ä¼¼æ€§æ£€ç´¢ï¼‰
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name=RAG_CONFIG["embedding_model"]
            )
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.embeddings = None
    
    def search_academic_sources(self, 
                              topic: str, 
                              sources: List[str] = ["web_search"],
                              max_results_per_source: int = None,
                              agent_role: str = "") -> List[SearchResult]:
        """
        æœç´¢å­¦æœ¯æ•°æ®æºï¼ˆä½¿ç”¨è”ç½‘æœç´¢ï¼Œé›†æˆJSON Modeï¼‰
        
        Args:
            topic: æœç´¢ä¸»é¢˜
            sources: æ•°æ®æºåˆ—è¡¨
            max_results_per_source: æ¯ä¸ªæ•°æ®æºçš„æœ€å¤§ç»“æœæ•°ï¼ˆç”¨æˆ·å¯é…ç½®ï¼‰
            agent_role: ä¸“å®¶è§’è‰²ï¼ˆç”¨äºå®šåˆ¶åŒ–åˆ†æï¼‰
        """
        
        if max_results_per_source is None:
            max_results_per_source = RAG_CONFIG["max_results_per_source"]
        
        print(f"ğŸ” JSON Modeè”ç½‘å­¦æœ¯æœç´¢é…ç½®ï¼šæœ€å¤š{max_results_per_source}ç¯‡ï¼Œè§’è‰²å®šåˆ¶ï¼š{agent_role}")
        
        # å‚æ•°å®‰å…¨æ£€æŸ¥
        if not topic or not topic.strip():
            print("âš ï¸ æœç´¢ä¸»é¢˜ä¸ºç©º")
            return []
        
        if not sources:
            sources = ["web_search"]  # é»˜è®¤ä½¿ç”¨è”ç½‘æœç´¢
        
        # æ£€æŸ¥ç¼“å­˜
        try:
            cached_results = self.cache.get_cached_results(topic, sources)
            if cached_results:
                print(f"âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ: {len(cached_results)} ç¯‡æ–‡çŒ®")
                # å¦‚æœæœ‰è§’è‰²ä¿¡æ¯ï¼Œé‡æ–°æ’åºä»¥é€‚åˆè¯¥è§’è‰²
                if agent_role and self.enhancer:
                    try:
                        cached_results = self.enhancer.enhance_results(cached_results, topic, agent_role)
                    except Exception as e:
                        print(f"âš ï¸ ç¼“å­˜ç»“æœå¢å¼ºå¤±è´¥: {e}")
                return cached_results
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ£€æŸ¥å¤±è´¥: {e}")
        
        all_results = []
        
        # è”ç½‘æœç´¢ (JSON Mode)
        if "web_search" in sources or "kimi" in sources:  # å…¼å®¹æ—§çš„æ ‡è¯†
            try:
                search_results = self.academic_searcher.search(topic, max_results_per_source, agent_role)
                all_results.extend(search_results)
                print(f"ğŸŒ JSON Modeè”ç½‘æœç´¢æ‰¾åˆ° {len(search_results)} ç¯‡æ–‡çŒ®ï¼ˆè®¾ç½®ä¸Šé™ï¼š{max_results_per_source}ç¯‡ï¼‰")
                
            except Exception as e:
                print(f"âŒ JSON Modeè”ç½‘æœç´¢å‡ºé”™: {e}")
        
        # ä½¿ç”¨LLMå¢å¼ºç»“æœï¼ˆè€ƒè™‘ä¸“å®¶è§’è‰²ï¼‰
        if all_results and self.enhancer:
            try:
                print(f"ğŸ¤– ä½¿ç”¨AIåˆ†ææœç´¢ç›¸å…³æ€§{'ï¼ˆä¸º' + agent_role + 'å®šåˆ¶ï¼‰' if agent_role else ''}...")
                all_results = self.enhancer.enhance_results(all_results, topic, agent_role)
            except Exception as e:
                print(f"âš ï¸ LLMå¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç»“æœ: {e}")
        
        # ç¼“å­˜ç»“æœ
        if all_results:
            try:
                self.cache.cache_results(topic, sources, all_results)
                print(f"ğŸ’¾ ç¼“å­˜äº† {len(all_results)} ç¯‡æ–‡çŒ®")
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
        
        return all_results
    
    def get_rag_context_for_agent(self, 
                                 agent_role: str, 
                                 debate_topic: str, 
                                 max_sources: int = 3,
                                 max_results_per_source: int = 2,
                                 force_refresh: bool = False) -> str:
        """
        ä¸ºç‰¹å®šè§’è‰²è·å–åŸºäºè”ç½‘æœç´¢çš„RAGä¸Šä¸‹æ–‡ (JSON Mode)
        
        Args:
            agent_role: ä¸“å®¶è§’è‰²
            debate_topic: è¾©è®ºä¸»é¢˜
            max_sources: æœ€å¤§å‚è€ƒæ–‡çŒ®æ•°ï¼ˆæ¥è‡ªç”¨æˆ·è®¾ç½®ï¼‰
            max_results_per_source: æ¯ä¸ªæ•°æ®æºçš„æœ€å¤§æ£€ç´¢æ•°
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
        """
        
        print(f"ğŸ” ä¸ºä¸“å®¶{agent_role}JSON Modeè”ç½‘æœç´¢å­¦æœ¯èµ„æ–™ï¼Œæœ€å¤§æ–‡çŒ®æ•°{max_sources}ç¯‡")
        
        # å‚æ•°å®‰å…¨æ£€æŸ¥
        if not agent_role or not debate_topic:
            print("âš ï¸ ä¸“å®¶è§’è‰²æˆ–è¾©è®ºä¸»é¢˜ä¸ºç©º")
            return "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚"
        
        if max_sources <= 0:
            print("âš ï¸ æœ€å¤§æ–‡çŒ®æ•°è®¾ç½®æ— æ•ˆ")
            return "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚"
        
        # å¦‚æœä¸å¼ºåˆ¶åˆ·æ–°ï¼Œå…ˆæ£€æŸ¥ä¸“å®¶ç¼“å­˜
        if not force_refresh:
            try:
                cached_context = self.cache.get_agent_cached_context(agent_role, debate_topic)
                if cached_context:
                    cached_ref_count = cached_context.count('å‚è€ƒèµ„æ–™')
                    print(f"ğŸ“š ä½¿ç”¨ä¸“å®¶ {agent_role} çš„ç¼“å­˜å­¦æœ¯èµ„æ–™ï¼š{cached_ref_count}ç¯‡")
                    
                    # å¦‚æœç¼“å­˜çš„æ•°é‡ä¸ç¬¦åˆç”¨æˆ·å½“å‰è®¾ç½®ï¼Œé‡æ–°æ£€ç´¢
                    if cached_ref_count != max_sources:
                        print(f"ğŸ”„ ç¼“å­˜æ–‡çŒ®æ•°({cached_ref_count})ä¸ç”¨æˆ·è®¾ç½®({max_sources})ä¸ç¬¦ï¼Œé‡æ–°æœç´¢...")
                    else:
                        return cached_context
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜æ£€æŸ¥å¤±è´¥: {e}")
        
        # åŸºäºè§’è‰²è°ƒæ•´æœç´¢æŸ¥è¯¢
        try:
            role_focused_query = self._create_role_focused_query(agent_role, debate_topic)
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ä¸»é¢˜: {e}")
            role_focused_query = debate_topic
        
        # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„æ•°é‡è¿›è¡Œè”ç½‘æœç´¢ (JSON Mode)
        try:
            results = self.search_academic_sources(
                role_focused_query, 
                sources=["web_search"],  # ä½¿ç”¨è”ç½‘æœç´¢ä½œä¸ºæ•°æ®æº
                max_results_per_source=max_sources,  # ç›´æ¥ä½¿ç”¨ç”¨æˆ·è®¾ç½®
                agent_role=agent_role
            )
        except Exception as e:
            print(f"âŒ JSON Modeè”ç½‘æœç´¢å¤±è´¥: {e}")
            return "è”ç½‘æœç´¢é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œè¯·åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å‘è¡¨è§‚ç‚¹ã€‚"
        
        if not results:
            context = "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚"
        else:
            try:
                # é€‰æ‹©ç”¨æˆ·è®¾ç½®æ•°é‡çš„æ–‡çŒ®
                top_results = results[:max_sources]
                
                print(f"ğŸ“Š JSON Modeè”ç½‘æœç´¢ç»“æœå¤„ç†ï¼šä¸ºä¸“å®¶ {agent_role} å®é™…æœç´¢åˆ° {len(results)} ç¯‡ï¼ŒæŒ‰ç”¨æˆ·è®¾ç½®é€‰æ‹©å‰ {len(top_results)} ç¯‡")
                
                # æ„å»ºä¸Šä¸‹æ–‡
                context_parts = []
                for i, result in enumerate(top_results, 1):
                    try:
                        context_part = f"""
å‚è€ƒèµ„æ–™ {i}:
æ ‡é¢˜: {result.title}
æ¥æº: {result.source} ({result.published_date})
å…³é”®å‘ç°: {result.key_findings or result.abstract[:200]}
ç›¸å…³æ€§: {result.relevance_score}/10
é“¾æ¥: {result.url}
"""
                        context_parts.append(context_part.strip())
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†ç¬¬{i}ç¯‡æ–‡çŒ®å¤±è´¥: {e}")
                        continue
                
                context = "\n\n".join(context_parts)
                
                # éªŒè¯æœ€ç»ˆç»“æœ
                final_ref_count = context.count('å‚è€ƒèµ„æ–™')
                print(f"âœ… JSON Modeè”ç½‘æœç´¢ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼š{final_ref_count}ç¯‡å‚è€ƒæ–‡çŒ®")
                
            except Exception as e:
                print(f"âŒ ä¸Šä¸‹æ–‡æ„å»ºå¤±è´¥: {e}")
                context = "è”ç½‘æœç´¢èµ„æ–™å¤„ç†é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œè¯·åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å‘è¡¨è§‚ç‚¹ã€‚"
        
        # ç¼“å­˜ç»“æœ
        if context and context != "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚":
            try:
                self.cache.cache_agent_context(agent_role, debate_topic, context)
            except Exception as e:
                print(f"âš ï¸ ä¸Šä¸‹æ–‡ç¼“å­˜å¤±è´¥: {e}")
        
        return context
    
    def _create_role_focused_query(self, agent_role: str, debate_topic: str) -> str:
        """åŸºäºè§’è‰²åˆ›å»ºé’ˆå¯¹æ€§æŸ¥è¯¢"""
        try:
            role_keywords = {
                "environmentalist": "ç¯å¢ƒä¿æŠ¤ æ°”å€™å˜åŒ– å¯æŒç»­å‘å±• ç”Ÿæ€å½±å“",
                "economist": "ç»æµå½±å“ æˆæœ¬æ•ˆç›Š å¸‚åœºåˆ†æ ç»æµæ”¿ç­–",
                "policy_maker": "æ”¿ç­–åˆ¶å®š ç›‘ç®¡æªæ–½ æ²»ç†æ¡†æ¶ å®æ–½ç­–ç•¥",
                "tech_expert": "æŠ€æœ¯åˆ›æ–° æŠ€æœ¯å¯è¡Œæ€§ æŠ€æœ¯å‘å±• æŠ€æœ¯å½±å“",
                "sociologist": "ç¤¾ä¼šå½±å“ ç¤¾ä¼šå˜åŒ– ç¤¾ç¾¤æ•ˆåº” ç¤¾ä¼šå…¬å¹³",
                "ethicist": "ä¼¦ç†é“å¾· é“å¾·è´£ä»» ä»·å€¼è§‚å¿µ ä¼¦ç†æ¡†æ¶"
            }
            
            keywords = role_keywords.get(agent_role, "")
            focused_query = f"{debate_topic} {keywords}".strip()
            print(f"ğŸ¯ ä¸º{agent_role}å®šåˆ¶JSON Modeè”ç½‘æœç´¢æŸ¥è¯¢ï¼š{focused_query}")
            return focused_query
        except Exception as e:
            print(f"âš ï¸ è§’è‰²æŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {e}")
            return debate_topic
    
    def preload_agent_contexts(self, agent_roles: List[str], debate_topic: str, max_refs_per_agent: int = 3):
        """
        é¢„åŠ è½½æ‰€æœ‰ä¸“å®¶çš„è”ç½‘æœç´¢ä¸Šä¸‹æ–‡ (JSON Mode)
        
        Args:
            agent_roles: ä¸“å®¶è§’è‰²åˆ—è¡¨
            debate_topic: è¾©è®ºä¸»é¢˜
            max_refs_per_agent: æ¯ä¸ªä¸“å®¶çš„æœ€å¤§å‚è€ƒæ–‡çŒ®æ•°ï¼ˆç”¨æˆ·è®¾ç½®ï¼‰
        """
        
        if not agent_roles or not debate_topic:
            print("âš ï¸ ä¸“å®¶è§’è‰²åˆ—è¡¨æˆ–è¾©è®ºä¸»é¢˜ä¸ºç©º")
            return
        
        print(f"ğŸš€ å¼€å§‹ä¸º {len(agent_roles)} ä½ä¸“å®¶é¢„åŠ è½½JSON Modeè”ç½‘æœç´¢èµ„æ–™...")
        print(f"ğŸ“Š ç”¨æˆ·é…ç½®ï¼šæ¯ä¸“å®¶æœ€å¤š {max_refs_per_agent} ç¯‡å‚è€ƒæ–‡çŒ®")
        
        for agent_role in agent_roles:
            try:
                print(f"ğŸ” ä¸ºä¸“å®¶ {agent_role} ä½¿ç”¨JSON Modeè”ç½‘æœç´¢...")
                context = self.get_rag_context_for_agent(
                    agent_role=agent_role,
                    debate_topic=debate_topic,
                    max_sources=max_refs_per_agent,  # ä½¿ç”¨ç”¨æˆ·è®¾ç½®
                    max_results_per_source=2,
                    force_refresh=True  # å¼ºåˆ¶åˆ·æ–°ç¡®ä¿æœ€æ–°èµ„æ–™
                )
                
                if context and context != "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚":
                    actual_count = context.count('å‚è€ƒèµ„æ–™')
                    print(f"âœ… ä¸“å®¶ {agent_role} çš„JSON Modeè”ç½‘æœç´¢èµ„æ–™å·²å‡†å¤‡å°±ç»ªï¼š{actual_count}ç¯‡")
                else:
                    print(f"âš ï¸ ä¸“å®¶ {agent_role} æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™")
                
                # é¿å…APIé™åˆ¶
                time.sleep(3)
                
            except Exception as e:
                print(f"âŒ ä¸ºä¸“å®¶ {agent_role} é¢„åŠ è½½èµ„æ–™å¤±è´¥: {e}")
                continue
        
        print("âœ… æ‰€æœ‰ä¸“å®¶çš„JSON Modeè”ç½‘æœç´¢èµ„æ–™é¢„åŠ è½½å®Œæˆ")
    
    def clear_all_caches(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        try:
            self.cache.clear_agent_cache()
            # æ¸…ç†é€šç”¨ç¼“å­˜
            for filename in os.listdir(self.cache.cache_dir):
                if filename.endswith('.json') and not filename.startswith('agent_'):
                    try:
                        os.remove(os.path.join(self.cache.cache_dir, filename))
                    except Exception as e:
                        print(f"âš ï¸ åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {filename}, {e}")
            print("âœ… å·²æ¸…ç†æ‰€æœ‰ç¼“å­˜")
        except Exception as e:
            print(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
    
    def test_integration(self, 
                        agent_role: str = "tech_expert", 
                        debate_topic: str = "äººå·¥æ™ºèƒ½å¯¹æ•™è‚²çš„å½±å“",
                        test_configs: List[int] = [1, 3, 5]):
        """
        æµ‹è¯•JSON Modeè”ç½‘æœç´¢é›†æˆ
        
        Args:
            agent_role: æµ‹è¯•ä¸“å®¶è§’è‰²
            debate_topic: æµ‹è¯•è¾©è®ºä¸»é¢˜  
            test_configs: æµ‹è¯•çš„å‚è€ƒæ–‡çŒ®æ•°é‡åˆ—è¡¨
        """
        print("ğŸ§ª å¼€å§‹æµ‹è¯•Kimi JSON Modeè”ç½‘æœç´¢é›†æˆ...")
        
        for max_refs in test_configs:
            print(f"\nğŸ“‹ æµ‹è¯•é…ç½®ï¼šæ¯ä¸“å®¶{max_refs}ç¯‡å‚è€ƒæ–‡çŒ®")
            
            # æ¸…ç†ç¼“å­˜ç¡®ä¿é‡æ–°æ£€ç´¢
            try:
                self.cache.clear_agent_cache(agent_role)
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
            
            try:
                context = self.get_rag_context_for_agent(
                    agent_role=agent_role,
                    debate_topic=debate_topic,
                    max_sources=max_refs,  # æµ‹è¯•ç”¨æˆ·è®¾ç½®
                    force_refresh=True
                )
                
                if context and context != "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚":
                    actual_count = context.count('å‚è€ƒèµ„æ–™')
                    status = "âœ…" if actual_count == max_refs else "âŒ"
                    print(f"{status} ç»“æœï¼šå®é™…{actual_count}ç¯‡ï¼ŒæœŸæœ›{max_refs}ç¯‡")
                    
                    if actual_count != max_refs:
                        print(f"âš ï¸ é…ç½®ä¸ç”Ÿæ•ˆï¼è¯·æ£€æŸ¥ä»£ç ")
                    
                    # æ˜¾ç¤ºç®€çŸ­å†…å®¹é¢„è§ˆ
                    print(f"ğŸ“„ å†…å®¹é¢„è§ˆï¼š{context[:200]}...")
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ°å­¦æœ¯èµ„æ–™")
                    
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        
        print("\nğŸ‰ Kimi JSON Modeè”ç½‘æœç´¢æµ‹è¯•å®Œæˆï¼")

# å…¨å±€RAGå®ä¾‹ï¼ˆå°†åœ¨graph.pyä¸­åˆå§‹åŒ–ï¼‰
rag_module = None

def initialize_rag_module(llm: ChatDeepSeek) -> DynamicRAGModule:
    """åˆå§‹åŒ–RAGæ¨¡å—ï¼ˆåŸºäºKimi APIè”ç½‘æœç´¢ï¼Œé›†æˆJSON Modeï¼‰"""
    global rag_module
    try:
        rag_module = DynamicRAGModule(llm)
        print("ğŸ” RAGæ¨¡å—å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨Kimi APIè”ç½‘æœç´¢åŠŸèƒ½ (JSON Mode)")
        return rag_module
    except Exception as e:
        print(f"âŒ JSON Modeè”ç½‘æœç´¢RAGæ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

def get_rag_module() -> Optional[DynamicRAGModule]:
    """è·å–RAGæ¨¡å—å®ä¾‹"""
    return rag_module

# æµ‹è¯•å‡½æ•°
def test_rag_module():
    """æµ‹è¯•åŸºäºKimi JSON Modeè”ç½‘æœç´¢çš„RAGæ¨¡å—åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•åŸºäºKimi JSON Modeè”ç½‘æœç´¢çš„RAGæ¨¡å—...")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("KIMI_API_KEY"):
        print("âŒ è­¦å‘Š: KIMI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼šexport KIMI_API_KEY=your_api_key")
        return
    
    # åˆ›å»ºæµ‹è¯•LLMï¼ˆéœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥ï¼‰
    try:
        from langchain_deepseek import ChatDeepSeek
        test_llm = ChatDeepSeek(model="deepseek-chat", temperature=0.3)
        
        # åˆå§‹åŒ–RAGæ¨¡å—
        rag = initialize_rag_module(test_llm)
        
        if not rag:
            print("âŒ JSON Modeè”ç½‘æœç´¢RAGæ¨¡å—åˆå§‹åŒ–å¤±è´¥")
            return
        
        # æµ‹è¯•ä¸“å®¶è§’è‰²è”ç½‘æœç´¢
        test_topic = "äººå·¥æ™ºèƒ½å¯¹å°±ä¸šçš„å½±å“"
        test_roles = ["tech_expert", "economist", "sociologist"]
        
        print("ğŸ” æµ‹è¯•åŸºäºKimi JSON Modeè”ç½‘æœç´¢çš„ä¸“å®¶è§’è‰²æ–‡çŒ®æ£€ç´¢...")
        for role in test_roles:
            # æµ‹è¯•ä¸åŒçš„ç”¨æˆ·é…ç½®
            for max_refs in [1, 3]:
                print(f"\nğŸ“Š æµ‹è¯•ï¼š{role} è·å– {max_refs} ç¯‡æ–‡çŒ®")
                try:
                    context = rag.get_rag_context_for_agent(
                        agent_role=role, 
                        debate_topic=test_topic,
                        max_sources=max_refs,  # æµ‹è¯•ç”¨æˆ·è®¾ç½®
                        force_refresh=True
                    )
                    
                    if context and context != "æš‚æ— ç›¸å…³å­¦æœ¯èµ„æ–™ã€‚":
                        actual_count = context.count('å‚è€ƒèµ„æ–™')
                        status = "âœ…" if actual_count == max_refs else "âŒ"
                        print(f"{status} ç»“æœï¼šæœŸæœ›{max_refs}ç¯‡ï¼Œå®é™…{actual_count}ç¯‡")
                        print(f"å‰100å­—ç¬¦ï¼š{context[:100]}...")
                    else:
                        print("âš ï¸ æœªæ‰¾åˆ°å­¦æœ¯èµ„æ–™")
                except Exception as e:
                    print(f"âŒ æµ‹è¯•å‡ºé”™: {e}")
        
        # ä¸“é—¨çš„JSON Modeè”ç½‘æœç´¢æµ‹è¯•
        print("\nğŸ”§ ä¸“é—¨æµ‹è¯•JSON Modeè”ç½‘æœç´¢åŠŸèƒ½...")
        try:
            rag.test_integration()
        except Exception as e:
            print(f"âŒ JSON Modeè”ç½‘æœç´¢æµ‹è¯•å¤±è´¥: {e}")
            
    except Exception as e:
        print(f"âŒ JSON Modeè”ç½‘æœç´¢RAGæ¨¡å—æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    test_rag_module()